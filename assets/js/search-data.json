{"0": {
    "doc": "Discussion - Mutation vs Real Faults and Daikon",
    "title": "Discussion: Test Oracles and Adequacy",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/10-mutation-daikon#discussion-test-oracles-and-adequacy",
    
    "relUrl": "/lecture-notes/10-mutation-daikon#discussion-test-oracles-and-adequacy"
  },"1": {
    "doc": "Discussion - Mutation vs Real Faults and Daikon",
    "title": "Agenda:",
    "content": ". | Administrivia . | Paper Proposal Feedback | Updated readings for next Tuesday | Flaky test and TotT follow-up | . | Are mutants a valid substitute for real faults? | Dynamically Discovering Likely Program Invariants to Support Program Evolution | . These notes roughly capture the (mostly unedited) comments from the class: . Are mutants a valid substitute for real faults? . | Why do we need to know the answer to this question? . | We want to use mutation to assess the fault-finding ability of our tests - can we? | . | Should it be: “are some mutants a valid substitute”? | What is hard about answering this question? . | How to gather the dataset? . | What is a “bug fix”? - Approach is to manually isolate the part of a commit that fixes a bug | . | How to control for code coverage? . | There could be a strong bias between code coverage and mutation | . | How to run this experiment? - lots of CPU resources required, probably produces lots of output | How to get a broad enough dataset so that this might generalize? | . | Overall experimental design? . | Collect a bunch of isolated bugs | Collect tests that reveal those bugs | Generate more tests | Do mutation analysis on all of the test suites | Does a test suite that detects more mutants detect more faults? | . | What is their definition of a “real fault?” . | Start with 5 open source projects . | Have issue tracker, have good version control | . | Look for bug-fix commits . | Manually inspect them | Discard faults that were not reproducible on the buggy commit . | What was the procedure to determine reproducible? Required that a test existed already | Are mutants a valid substitute for faults that developers don’t write test cases for? | … are mutants a valid substitute for faults in distributed systems caused by exception handling bugs (OSDI 2014)? | . | Ensured that there was only a single change in the commit (tried to manually do this) | Discard commits where the only changes were to tests | . | . | Where do developer tests come from? . | (T_pass), (T_fail) For a specific commit, T_pass is the test suite that passes, T_fail differs by exactly one test case that reveals the test case (if there are multiple failing tests, it’s multiple pairs) | . | What is a mutant? . | What operators? . | Replace constants | Replace operators | Modify branch conditions | Delete statements | . | Where to apply mutations? . | Applied to the classes that had been changed by the bug fix . | Didn’t want to create a lot of “useless” mutants - OK to save time to do fewer | Assumption: a mutant in the same location as the bug will represent that bug . | Null hypothesis is that there is no correlation. Making more mutants could only IMPROVE the strength of the finding that there is a correlation | . | . | . | . | Run the experiments . | How to control for code coverage? . | Only consider mutants that are covered by all of the tests | . | RQ: Are real faults coupled to mutants generated by commonly used mutation operators? . | If mutants are valid substitute, then any test suite with a higher fault detection rate should also have higher mutation score - “coupled” mutants and real faults | . | RQ: What types of real faults are not coupled? . | Manual analysis | . | RQ: Is mutant detection correlated with real fault detection . | Generate tests . | Need more tests to make a conclusion if mutation score of a tests suite is actually correlated | Used three different tools: EvoSuite, Randoop, JCrasher. | . | Compare mutation scores between test suites | . | . | What are results? . | Are real fault scowled to mutants generated by commonly used mutation operators? . | 73% were coupled | . | What was not coupled? . | Some, they believe, can not be coupled: . | “Incorrect algorithms” - cases where an algorithm had to be changed | . | “Similar library method called” | Some faults required stronger mutation operators: . | Statement deletion | Argument swapping | Argument omission | Similar library method called | . | Here’s a less strong mutation operator: . | x = x + 0 | . | . | Is mutant detection correlated with real fault detection? . | Yes | Also, correlation between mutation detection and real fault detection stronger than coverage vs real fault detection | . | . | Reactions . | “are some mutants a valid substitute?” | Extremely careful and time-consuming methodology, come to valid and sound conclusions for the dataset, but does this dataset generalize? . | More recent work shows that this still holds on bigger datasets built with same methodology, but what about changing the methodology? (What we consider a bug… also…. Languages? Just Java) | New context: bugs = student implementation of an assignment that failed an instructor-provided test . | C, TypeScript | . | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/10-mutation-daikon#agenda",
    
    "relUrl": "/lecture-notes/10-mutation-daikon#agenda"
  },"2": {
    "doc": "Discussion - Mutation vs Real Faults and Daikon",
    "title": "Dynamically Discovering Likely Program Invariants to Support Program Evolution",
    "content": ". | What is the motivation? . | There are implicit invariants in a programmer’s mind while coding, the program could be more robust if they were encoded . | Example: i should monotonically increase in a for loop, or the bound of a loop should be set at som point | . | . | What makes it a likely program invariant? . | Based on the tests, it was always true, but the tests might not be comprehensive | Are they “likely” or “possible”? . | They define “likely” based on a probabilistic assumption that values are uniformly randomly distributed | Example: if x==3 on every execution, but x is a 32-bit integer, it’s very unlikely that it would have happened every time | Case for possible: . | Input a value in the range [3,4] -&gt; x | . | Case for likely: . | “Likely” based on developer acceptance | . | . | . | What is the use-case for program evolution? . | If the maintainer becomes aware of the invariants, they will accept them, add them to code, and then this will help future developers/same developer to maintain the code | Can use this to generate invariants on-demand for specific regions, too | Maybe useful for fuzz testing: use as a fitness function to evaluate closeness to finding bugs. Or, in the 1999 terminology, good for finding edge cases :) | Assertions are helpful for evolution | There is a narrative explaining how this was helpful for augmenting a regex compiler | Would it be helpful to prune search space for model checking | . | What are the challenges in detecting invariants? . | Need to find the ones that are useful . | What’s not a useful invariant? . | const x = 3; //assert(x==3) assert(x!=4) | “This list is not sorted” | Probabilities, too… | . | . | Where do the invariants come from? . | Record a lot of things | Establish patterns for invariants . | They are established in the paper | Only detect invariants for which there is a template | . | Establish a threshold for “negative” invariants | Only consider possible invariants that include at most 3 variables? . | It will take a long time to run | . | . | Implementation details . | Frontend/backend distinction . | Frontend - records trace | Backend - infers likely invariants | Benefits? . | Can add new patterns and re-use the existing trace and frontend | Can support multiple languages, just reuse the backend | . | Could it have been more efficient to put frontend + backend together? What do we lose? . | Can’t infer while tracing - no feedback loop possible | Stuck with variables that can be serialized into the trace format | . | . | . | Evaluation . | What was evaluated? . | On the Gries program - do the inferred invariants match the textbook invariants? Yes | What were these “Gries programs”? . | Examples for teaching programming and invariants, demonstrate that it can find those “easy” invariants | . | Scalability . | How long it takes to run the tracer | How long it takes to find the invariants . | 450MhZ Pentium II | “The engine has not been optimized for time and space” | “We used interpreted language Python” | . | Relationship between number of tests and number of invariants | . | . | Future studies? . | How many invariants are useful vs useless? (User study) . | Of the invariants detected, who accepts them as useful/valid | . | . | Inspirational? . | Brilliant! Especially for large programs | Having examples can be a useful tool for developers - showing what values might be in different places in a program is useful | Still used/maintained, 30 years later! | . | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/10-mutation-daikon#dynamically-discovering-likely-program-invariants-to-support-program-evolution",
    
    "relUrl": "/lecture-notes/10-mutation-daikon#dynamically-discovering-likely-program-invariants-to-support-program-evolution"
  },"3": {
    "doc": "Discussion - Mutation vs Real Faults and Daikon",
    "title": "Discussion - Mutation vs Real Faults and Daikon",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/10-mutation-daikon",
    
    "relUrl": "/lecture-notes/10-mutation-daikon"
  },"4": {
    "doc": "Discussion - Fuzzing",
    "title": "Discussion: Fuzzing - “Automated Testing of Graphics Shader Compilers.” (Donaldson et al 2017)",
    "content": "Poll: . To what extent do you think that this work could impact SE? Not very: 0 Unsure/so-so: 5 Very: 15 . | Coming up with those semantics-preserving changes for new langauges/domains can be tricky (also finding ones that are defect-revealing) | Having multiple implementations of shader compilers gives “more bang for buck” - find bugs in multiple implementations concurrently | An impressive effort | . To what extent do you think that the article is convincing that this is the best approach for testing graphics shader compilers? Not very: 0 Unsure/so-so: 8 Convinced: 11 . | Are there potential inconsistencies/biases based on the shaders that they used as inputs - would a qualitative analysis of the shaders help provide more confidence? | “High-value” bugs are a good target | . Context and background . | Why are graphics shader compilers difficult to test? . | No “correct” answer | . | Why not do “differential” testing - compare “fuzzily” the output of two compilers? . | Requires multiple GPUs w/ benchmarks - need a precise definition of “equivalent” | Still need to determine which one has the bug | Vendor-specific extensions prevent this to be used fully | . | What is the relationship between this work and testing approximate computing? . | What is the rigorous definition of “close”? For shader compiler fuzzing, we might need to have a human evaluator (“the fog didn’t render but that was OK”) | . | . Metamorphic testing . Example: sin(x) we want to test, we don’t know what the outputs should be . Metamorphic relation: Specify some property that describes a transformation of the input, and ac corresponding transformation on the output. | sin(-x)=-sin(x) | sin(2pi+x) = sin(x) | integral from 0 to 2pi of sin(x) = 0 | . What does this buy us in terms of being able to test things, example, sin function? . | Provides a relationship from function inputs to outputs . | Create “follow-on” inputs | . | What is metamorphic relation in this context of GLFuzz? . | Image generated by shader should be equivalent to the one generated by a shader with a semantics-preserving transformation applied | . | What other domains might we be able to specify metamorphic relationships for, and maybe use to find bugs? . | Machine learning - There are a lot of matrix multiplications, do relations over the multiplications. | SQL queries - semantics preserving transformations, or even some that are not preserving, but we understand (e.g. LIMIT 10 should restrict number of rows) | Testing for data races - add no-ops into code might expose a data race (aka the semantics-preserving transformation) | Numerical analysis - know the different transformations, what is acceptable in terms of approximation | (Generally: computer algebra systems) | Image/video processing - semantics preserving transformations, but also identity transformations (crop it, should still be similar, etc) | Compiler testing - same as this paper. Property: semantics-preserving change to program should yield same compiled program/output | . | . What is motivation? . | Why do we care about shader compilers? . | It produces nice graphics, looks good in a powerpoint | It’s also a nice research problem, because there is no easy/obvious way to do it | . | All of the poor gamers need games to work right; safety. | Safety can be both from security perspective (exploit-freedom) and general correctness (crash-freedom) | Tesla almost definitely is using OpenGL :) | . What is the solution? . Apply semantics-preserving transformation to some inputs, run them, see if output is the same: compileAndRun(P, I) = compileAndRun(f(P(I), I), where f(x) produces a semantically-equivalent program . Given an input set of shaders… . | How do they generate the variants? . | Add dead-code of various kinds: x = 1; if(x == 0){} . | Don’t just put empty blocks in there - copy/paste code from other shaders, break or a continue, just return | . | “Live code injection” . | Copies some code that computes some local variables from one shader into another | Why copy code from another shader instead of generating from scratch? . | There is some risk that what you generate could have some unexpected side-effect | It might be hard | Maybe it would be more likely to create something that would be recognized as a “real” bug | . | . | Mutate code with mathematical identities, x=1*x, x=0 + x | Pack variables into a vector . | x,y,z -&gt; q = vec(x,y,z) | . | Q: Need to ensure that the control flow of the program is preserved so that it doesn’t unexpectedly crash (aka they make non-semantics-preserving changes ) . | Is there a guarantee that the changes are semantics-preserving? (No) | What is the risk if we make a non-semantics-preserving change? . | False positives (report a bug when there isn’t) | . | Every transformation needs to be reversible for minification | . | Detecting “deviant” variations . | Some heuristic for determining “similarity” | Build a histogram of the RGB distributions of each image | Compare distributions to determine if they are statistically different (chi-squared distance) | . | Reduce/minifify the deviant variations . | Reverse a random set of transformations, if not equivalent to original image, repeat until converges to local maxima | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/12-glfuzz#discussion-fuzzing---automated-testing-of-graphics-shader-compilers-donaldson-et-al-2017",
    
    "relUrl": "/lecture-notes/12-glfuzz#discussion-fuzzing---automated-testing-of-graphics-shader-compilers-donaldson-et-al-2017"
  },"5": {
    "doc": "Discussion - Fuzzing",
    "title": "Evaluation",
    "content": ". | What scale of bugs per-dollar should we be accepting of? . | If we are Nvidia/arm…. :/ | If we are a platform vendor, maybe we care a lot | If we are an application developer in, say, the medical domain, we might want/need to have some higher degree of certainty of whether our software generates correct visualizations on different hardware | . | 17 devices, 3 evaluation sets | 1,000 shaders from GSLSSandbox.com, took 100 largest shaders that could be compiled | How do they choose to apply which transformation? . | Randomly - “swarm testing” | . | Identify false positives . | Three fellow researchers, not involved in the project, indicate via a key-press whether each pair is visually distinguishable (how long?) | . | Table 3 - the true positives and false positives by technique . | Mutation is very productive, but it also is going to be most likely to introduce floating point errors that result in false positives | Could false positives be reduced? Does it matter? | . | . Overall discussion . | How to focus on detecting security vulnerabilities? . | Identify shaders that are executing memory-risky parts of code? | Have no interest in checking the output images . | But, having a frame-by-frame analysis is still important/valuable for finding those security issues | . | . | Faster/easier way to get the same results? . | How to make this a generic approach/architecture for testing compilers, with “semantics preserving changes?” . | Need some engine for rewriting the code | Need some checker to determine whether the output of the programs is the same . | Maybe easy, maybe hard? | . | Need to be able to clearly specify the “semantics-preserving” part | . | . | Is it valuable to find bugs in older software that may have been fixed already? . | Yes: it shows that approach works. Maybe one could argue that this approach is “cheaper”? | And it means that developers really care about the bugs! | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/12-glfuzz#evaluation",
    
    "relUrl": "/lecture-notes/12-glfuzz#evaluation"
  },"6": {
    "doc": "Discussion - Fuzzing",
    "title": "Discussion - Fuzzing",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/12-glfuzz",
    
    "relUrl": "/lecture-notes/12-glfuzz"
  },"7": {
    "doc": "Discussion - CI",
    "title": "Discussion: Continuous Integration",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/14-ci#discussion-continuous-integration",
    
    "relUrl": "/lecture-notes/14-ci#discussion-continuous-integration"
  },"8": {
    "doc": "Discussion - CI",
    "title": "Summary of opinions and perceptions",
    "content": "Tradeoffs in CI: significance . | Not very: 3 | Unsure: 6 | Very: 10 Are these tradeoffs that could already be known in the small? Tradeoffs in CI: Empirically sound Not very: 0 | Unsure: 11 | Very: 8 Taming Google CI: significance within google? Not very: 0 | Unsure: 4 | Very: 15 Taiming Google CI: significance outside of google? Not very:2 | Unsure: 10 | Very: 7 | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/14-ci#summary-of-opinions-and-perceptions",
    
    "relUrl": "/lecture-notes/14-ci#summary-of-opinions-and-perceptions"
  },"9": {
    "doc": "Discussion - CI",
    "title": "Tradeoffs in CI",
    "content": ". | Methodology . | Interviews . | 16 participants | Open-ended | Coded results | . | Survey . | Broad population: distribute on twitter, Facebook, reddit | Focused population: Target developers at pivotal | . | . | . Barriers &amp; ideas . | Troubleshooting CI build failures . | Some may have nothing to do with CI itself | Something that makes debugging CI easier: being able to reproduce same environment, and/or connect to it | . | Overly long build times . | Coffee break, lunch break, sleep break, weekend, winter break | Developer approach: ignore some unlikely to fail tests | “Regression test selection”: Compute for each test: which statements it covers | . | Automating the build process | Lack of support for desired workflow . | What is organizational culture to build support for these kinds of workflows? Tied to awareness of what CI can perform | Today: GitHub actions workflow.yml, action.yml | . | Security and access control: . | Access to “secrets” (aka API key) . | Vulnerability: secret is publicly readable | Vulnerability/attack path: untrusted code could be merged in and have access to that secret | . | . | . Needs: . | Easier configuration of CI servers or services . | Bamboo :’( | There is usually a big range of skills needed between the technology that is being tested and the technology used for testing | . | Better tool integration . | “There are probably CIs that already do that, but ours doesn’t” | . | Better container/virtualization support | Debugging assistance | UI for modifying CI configs | Better notifications . | “I get 10 emails and I delete them immediately” | . | Making it easier to onboard new users to the platform . | How to extend existing workflows and create new ones | If the process is bad enough, then no tool can help you. Bad practices in dev processes get baked into CI processes get baked into CI tools. | What is the current state of best-practices in CI? | . | . Discussion . | Any surprising insights, even given past 5 years time? . | Using CI might be the difference between people actually running the tests and it not happening | . | Security restriction in CI tools . | The workarounds that devs use for running systems and debugging them in CI are not optimal… | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/14-ci#tradeoffs-in-ci",
    
    "relUrl": "/lecture-notes/14-ci#tradeoffs-in-ci"
  },"10": {
    "doc": "Discussion - CI",
    "title": "Taming Google Scale Continuous Testing",
    "content": ". | Bazel | Monorepo | Declare dependencies on other modules | When going to test module A, collect all “affected” modules of module A, run their tests | Commits -&gt; “changelist” . | “Milestones” optimize when to choose to run which tests | Frequently modified source files are more often to be involved in test breakages . | “You are modifying a file that was modified 41 times in the past. The probability that you will cause a breakage is 75%.” | Maybe for these files, you go deeper in dependency graph | Does separating them into smaller components reduce the impact of this? | Maybe you run these more often. | More defect-inducing files should be less likely to put into same milestone | . | What are the unknown unknowns (the causes of the failures?) | Impressed by the number of bugs found by downstream test running | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/14-ci#taming-google-scale-continuous-testing",
    
    "relUrl": "/lecture-notes/14-ci#taming-google-scale-continuous-testing"
  },"11": {
    "doc": "Discussion - CI",
    "title": "Discussion - CI",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/14-ci",
    
    "relUrl": "/lecture-notes/14-ci"
  },"12": {
    "doc": "Discussion - DevOps",
    "title": "Discussion: DevOps (Canopy &amp; Evolutionary study of configuration in cloud)",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/17-devops#discussion-devops-canopy--evolutionary-study-of-configuration-in-cloud",
    
    "relUrl": "/lecture-notes/17-devops#discussion-devops-canopy--evolutionary-study-of-configuration-in-cloud"
  },"13": {
    "doc": "Discussion - DevOps",
    "title": "Agenda:",
    "content": ". | Administrative reminders . | Reflection paper feedback and grades posted, reminder of course grading policy: 3 checks -&gt; A, check+ doesn’t impact grade | Project status update due Tuesday, 11am | Weeks 13 and 14 readings updated as discussed on Tuesday | . | Follow-up and lingering questions from Tuesday’s DevOps lecture | Poll (https://PollEv.com/jbell) | Discussion: Canopy | Discussion: An Evolutionary Study of Configuration Design and Implementation in Cloud Systems | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/17-devops#agenda",
    
    "relUrl": "/lecture-notes/17-devops#agenda"
  },"14": {
    "doc": "Discussion - DevOps",
    "title": "Discussion: Canopy",
    "content": ". | What is the problem that they are solving? What is the world that this system lives in? . | What caused a crash? When you add 10 features at once, it is important to be able to precisely debug things | . | What is hard about this, and makes it not just Log4J? (Or any other logging framework) . | Performance debugging is hard - unobservable (make it observable) | Hard to correlate log statements, particularly across distributed system | What is the right level of granularity to use for logging: downsides are that too much takes too much space, too hard to understand | Logging is increasing attack surface | LOTS of concurrent debuggers, all trying to measure different things | . | What is the methodology for doing this debugging? . | Determine what it is potentially causing a problem, and then determine where to insert some tracing probes, and then insert them | . | What is the design space? . | Want to decouple: “How to determine what to log” from “what is the problem that we need to troubleshoot?” | Look to create a system for decision support for testing hypotheses | If we do not know where to put logging statements (based on the problem that occurs), where do they go? . | Propose putting them in core blocks/areas - the model is really “log everything” . | Provide an API | . | . | . | What are the downsides of “log everything”’ . | Hard to maintain performance – 1.16GB/sec | Capture a “Trace” . | For a request: when the request enters system, assign an ID, for every component that processes that request, store log entries, along with the ID | Directed acyclic graph with dependencies | “Trace is sharded across backend processing pipeline by TraceID” . | Can have a pool of 100 servers, assign a traceID to each server | After trace is captured, flushed to processor | SCUBA database stores the underlying log files | . | . | Hard to analyze/gain insights . | Start by isolating to analyze in specific components | Start by reducing the dataset (“feature extraction lambdas”) | . | How to use as a developer debugging? . | Examine data with extracted features, or create new feature extractors | . | . | Design requirements that they fit within: . | Don’t know in advance what we need to log or ask about | Don’t want to have to maintain a data format . | Not worth it in terms of space: but also in terms of coordination | . | Have to deal with a lot of different execution models . | Server vs browser vs iPhone . | Many requests on one server vs single thread of JS in browser vs device limitations | Constraints wrt bandwidth, power etc | . | . | . | Evaluation . | Canopy is a better tool than X? | Canopy doesn’t make my site too slow? | Canopy is useful? Do developers like it? . | Reliance on case studies | It seems like it works | How could you even do a user study? | Is it still in use? How has it evolved? | A lot of information is better than 0 information | What other questions would you ask to study the interface? . | If you were pitching Facebook: “Let’s make the interface for performance debugging better” what is that study? . | Find places where we found bugs before, do an A/B trial with the new and old interface to see ability to find the bug | Fix the hypotheses: tell the engineer “here is the hypotheses you are trying to test” and see how long it takes and with what accuracy the human can test that | Baseline: Compare aggregate traces to the raw ones? | . | . | . | Does Canopy scale to Facebook? . | Yes | “We tolerate more latency the further the latency is from us” . | OK to be worse on phone browser than on server | “It is worth it” if it solves bigger problems | . | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/17-devops#discussion-canopy",
    
    "relUrl": "/lecture-notes/17-devops#discussion-canopy"
  },"15": {
    "doc": "Discussion - DevOps",
    "title": "Discussion: An evolutionary study of configuration design and implementation in cloud systems",
    "content": "What are the big-picture questions that this study aims to answer? . | What is the current state of practice in the interface for configuring systems? | What are the objects that they study? . | 4 open-source applications . | The configuration interface | The usage of those configuration values within the application | The documentation of those configuration options | OK with it being all Apache? . | They are widely used | . | . | . | What is the overall story for significance? . | Configuration design is something that should be considered a first-class software engineering problem . | We should automatically check configurations after reading them in | Alarm-bell paper: “Facebook and google both had 15% of their failures come from misconfigurations!” But: developers of configurable software are not checking this carefully | . | . | Methodology: How to find changes to these projects that impact configuration values . | Keyword search for issues (and do manual examination) | Search the content of diffs for configuration-related keywords | Look for changes to program regions that use configurations | Might miss some “deep” aspects in the code that | . | . Not a question in this article: . | How do configuration files used in deployed systems evolve over time? . | Maybe could then improve the software as a result | Undocumented parameters are a horrible idea | What causes people to stray from the defaults? | . | Is it important to study configurability? . | Yes, because it goes to usability | But what systems? | What are actual configurations that induce failures? | . | What is the relationship of configurability between open source and proprietary software? . | What does the SQLite dev team think of this kind of configurability study? | . | What for next time? . | IaC over configuration? | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/17-devops#discussion-an-evolutionary-study-of-configuration-design-and-implementation-in-cloud-systems",
    
    "relUrl": "/lecture-notes/17-devops#discussion-an-evolutionary-study-of-configuration-design-and-implementation-in-cloud-systems"
  },"16": {
    "doc": "Discussion - DevOps",
    "title": "Discussion - DevOps",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/17-devops",
    
    "relUrl": "/lecture-notes/17-devops"
  },"17": {
    "doc": "Discussion - Expertise",
    "title": "Discussion: What makes a great software engineer &amp; Programming Strategies",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/19-expertise#discussion-what-makes-a-great-software-engineer--programming-strategies",
    
    "relUrl": "/lecture-notes/19-expertise#discussion-what-makes-a-great-software-engineer--programming-strategies"
  },"18": {
    "doc": "Discussion - Expertise",
    "title": "What makes a great software engineer?",
    "content": "Why is this a question that we should ask? . | It could be actionable: if we know what makes a software engineer great, we could create training and processes to develop those . | And when you notice in yourself, you could focus on that | (And focus hiring activities - especially in lower level positions where we are hiring based on potential) | Need to do a broad survey to capture not just technical attributes | Helps to recognize the limitations of software engineers, and understand what a reasonable ask is | Helps to inspire future technologies or tools to manage that development | Important to distinguish “software engineer” vs “programmer” | . | What is the difference between “what is a good software engineer” and “what is a good business worker?” . | See the programming strategies paper: much of the knowledge may be tacit | Technical skills are not needed in all jobs, but what is needed in this job is the ability to collaborate between those with technical skills and the non-technical/domain skills | It is easier to get outdated in the SE domain (or to simply have the “wrong” technical skills) | Connecting the value to the business/user to the technical work/code is hard | . | Is a great SE at Microsoft a great SE at Apple (…or Chewy or Wayfair), or elsewhere? . | Well, maybe… but the easiest way to determine this is to look at table 2 and determine whether it is adequate for the job or not | Different companies have different objectives, goals and aims. For small companies, especially startups engineers might need to handle a broader set of tasks. Also a big difference if the software is B2B vs B2C - different | Is this different based on level - startup prefers senior level engineer vs entry-level? | If the answer to this is no: then does that mean that one of those companies is doing it “wrong”? . | Especially at the high level of figure 1: The emphasis on non-technical skills should generalize | No, it means that they are just hiring programmers and not software engineers | . | . | What DOES make a great software engineer? . | Personal characteristics - are these all intrinsic? Are any? . | Ultimately, down to training - could be transferred through education, or through - but perhaps with greater time/resources/effort | Some of these are “hard lessons” | Some are very tied to the condition that you are in: adaptability (also passion) | Passion is king? Intrinsic rewards &gt; extrinsic rewards | Constant drive for improvement: how do you teach this? | There are conflicts between attributes: someone who always wants to improve might not be a great team player (depends on the team) | How to implement training for this, based on different individuals’ personalities/strengths | . | Team qualities . | Should we consider people who are not software engineers to understand what makes a great software engineer? | Potentially great software engineers might be introverts, and not sharing enough to create team success | How does WFH/remote work intersect with these qualities and day-to-day activities that could benefit these qualities (but are missed)? . | “What makes a great remote software engineer?” | . | How to recognize tacit knowledge, both to non-software engineers, but perhaps more importantly, within a team? | How to recognize things that can be tested (trial and error) vs must get right the first time (and have communication overhead up-front) | . | Decision making . | Most general and applicable to other fields of business, but least emphasized in CS education and hiring | A lot of this is learned through experience and is specific to the context/team | Can definitely teach at least some of these skills: “What kind of animal would you be?” (More serious things here) | Explicit education on leadership and collaboration. Here are the learning objectives: . | A good leader is one who can take a vacation on short notice, because their team can function in their absence . | Counterpoint: if you want to be promoted, then maybe you need to prove that in your absence, you are missed :) | . | A good collaborator is an effective communicator, who can explain what they did to different audiences | A good leader creates artifacts (e.g. documentation) to enable others to succeed in the leader’s tasks. Successful documentation is documentation that can be reused | A good leader is able to hire and supervise contributors smarter than them | A good leader is knowledgable AND is knowledgable about their team in order to effectively transfer tacit (and other) knowledge to them | A good leader has integrity | . | How to teach all of this? . | Communication - either first-hand experience, or learned experience | . | . | . | Aspects of the product . | Things that we do teach? | Things that we don’t do a great job teaching? . | Evolving/long-term, how to structure software to be efficiently delivered over time | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/19-expertise#what-makes-a-great-software-engineer",
    
    "relUrl": "/lecture-notes/19-expertise#what-makes-a-great-software-engineer"
  },"19": {
    "doc": "Discussion - Expertise",
    "title": "An exploratory study of sharing strategic programming knowledge",
    "content": ". | How does one great software engineer transfer (especially tacit) expertise to others? | What do we think about figure 1 strategies? . | KNOW WHO YOUR AUDIENCE IS | Need to understand the shared context - so that we can understand what tacit knowledge is known by the reader/user | . | Why do we have this tacit knowledge problem in software engineering? . | Evolution - write a book, and it’s obsolete | Maybe it’s not tacit, we are just not putting in the effort to explain it | Software engineering is a field that allows for trial and error in ways not possible in other fields. A trainee might “succeed” on an assignment but not learn that tacit knowledge | . | “If you really understand it then you can teach it to someone” - pedagogical content knowledge | Why Roboto? . | Shared grammar and expectations | Could an LLM execute the strategy? | . | Discussion . | Would study participants have invented something else if not using roboto? | This is overdue. | What about cross-referencing strategies, so that there are “details on demand” | Maybe debugging is not the right application for this - it could apply more broadly . | Or maybe it is :) | High level strategy: Hypothesis-driven debugging | Low level strategy: Create approaches to generate hypotheses | . | Relationship to expert systems? | We want this for cooking, and more english-y things | We also want to have some NLP techniques to seed the pot of strategies | What makes a great programming strategy author? . | Pedagogical content knowledge, plus…? | We need ChatGPT + programming strategies | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/19-expertise#an-exploratory-study-of-sharing-strategic-programming-knowledge",
    
    "relUrl": "/lecture-notes/19-expertise#an-exploratory-study-of-sharing-strategic-programming-knowledge"
  },"20": {
    "doc": "Discussion - Expertise",
    "title": "Discussion - Expertise",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/19-expertise",
    
    "relUrl": "/lecture-notes/19-expertise"
  },"21": {
    "doc": "Discussion - SDI",
    "title": "Discussion: Software Aspects of Strategic Defense Systems",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . | Hot Takes/First Impressions: What did you think of these essays, overall? . | Requirements: “Do everything perfectly” | Important/interesting to discuss education of programmers | Is this actually a problem of programming languages? | Most (or all?) of the problems discussed exist not only for the SDI, but in other contexts too | How to reason about what a reasonable/good investment is? . | Applied research . | Make the missile hit the exact target | . | Fundamental research . | Make software more reliable | What we needed then: concurrency, fault tolerant systems | . | . | Wow, we sure had much different hardware at the time, and the difference between a high and a low level programming language… | The hard requirements (never miss a target) are pretty insane . | Not. Ever. Possible. | . | We have had actual documented cases of missile detection systems providing false positives that would have resulted in nuclear war had not a human intervened | . | Essays on software process . | Why software is unreliable . | Continuous vs discrete states. If continuous states, can’t test all states in advance | It depends on what you want to prove and in which circumstances | How do we understand software - decompose it into modules? . | Is it possible for modules to break down, and lead to unreliable software? . | Specifying WHICH module to use can be a challenge | By construction, different modules must be built by different teams | Concurrency/multi-processing | Different modules built by different teams may be unreliable in themselves, especially if they are entirely disconnected in their development structure | . | . | Software engineers are not trained sufficiently . | Insufficient understanding of when different methods are applicable, and under what circumstances they can be expected to generalize/continue to perform | “Think like a computer” is not an appropriate teaching method . | Programming by trial-and-error (but this requires tests) | . | Not good enough tooling support | Higher level languages might help “think like a computer” by raising the abstraction level | Rigorous training in requirements writing and decomposition - understanding in particular implicit requirements for behaviors that should NOT occur | How much of this problem is related to EXPERIENCE that can’t be trained/shared, versus training? . | Prototyping/iterative decomposition | Patterns/architecture? | . | . | . | Why SDI will be untrustworthy . | “Untrustworthy” - this means not 100% perfect performance - should that just be “nuff said”? | This is realistically a systems problem, not just including the post-launch phase (intelligence gathering, pre-launch capabilities, other countermeasures) | Example: How to develop fire control software? What are the inputs? What are the algorithms to use? How to test this? . | Start with the given inputs: we have some ballistic missile that is on some trajectory | AI has gotten so much further | . | Many of the assumptions are grounded on the current state of hardware/software at the day | . | Why conventional SW development does not produce reliable programs | . | Essays on “emerging approaches” . | The limits of SE methods . | Waterfall process . | At a system-level, the entire system must be correct “the first time” | Requirements/interfaces between modules must be carefully gathered, designed, analyzed (Rely on experience building previous things to do this) | . | Structured programming | Formally specified interfaces . | What is an example of a difficult design decision that could be easily overlooked? . | Constraints of execution time, space requirements for modules must be specified ahead | Disconnect between project managers/engineers - particularly in terms of what is actually possible with current technology | Anything related to assumptions of the countermeasures of the enemy, all of which must be based on (uncertain) intelligence (plus the fact that there might be gas in that enemy software) | Implicit assumptions, e.g. on iteration over a hash table | . | Is a solution to this grounded in software, in humans, or in both? . | Some things can be solved in software. Especially discussion of multi-processing, baking these kinds of solutions into languages (e.g. Rust, async actors) | Some things need to be solved in human processes. | Software can be used to help mediate communication. | How to organize high-level design vs low-level design? | How to structure teams? | . | Maybe most/all things are socio-technical, and depending on the problem, might be more of one side than the other | Maybe we need to break bad habits? | . | . | Cooperative multiprocessing using locks/signals | The Software Cost Reduction project . | What was learned from this 8+ year effort? . | Need complete black-box requirements upfront | MODULES, and also why to use modules | Precise documentation using formal methods (compare to “functionality over documentation”) | . | “Every new concept requires at least 2 generations of engineers to become the standard” | . | . | AI and the SDI . | AI has gotten pretty far compared to what it was | “People speak of AI as if it were some magic body of new ideas” | We have found out that many things that we thought were hard (chess) are “easy” others (recognizing blocks) hard | . | Can automatic programming solve the SDI problem? . | Even if we have ChatGPT to generate code, still need to generate prompt | Helps programmer with the “simple” things, and as this is continued to be used, you eventually might generate more reliable code | GitHub CoPilot is a big “wow” but not the end of the story | Programs need to be read by humans also: are these automatic techniques making things that are not maintainable | What kind of explainability/traceability do we get from this? | What about new programming languages? . | Garbage collectors? | Programming by specification | Type systems | Would a higher level language lead to more reliable programs? Pros and cons… . | As more things are abstracted away, developer may become less aware of what is actually happening under the hood | How to conceptualize “real-time” - millisecond latency is OK | Performance is harder to reason about | The low-level mechanisms implemented by the high-level language should be correct | Higher-level problems may be easier to reason about in maintenace | Can actually spend time focusing on what it is that you need to do, rather than how to implement the low-level parts of it | . | How do programming environments improve reliability? . | What is a programming environment in this context? (Other than Unix™) . | Operating system, editors | . | What programming environments make software more reliable/faster to build? . | IDEs + language servers - include linting/test/analysis results directly inside of an integrated environment. Refactoring. | Simulation of physical devices in virtual environments | Version control + collaboration tools | Continuous integration | . | . | . | . | Can program verification make the SDI software reliable? . | What was missing at the time? Languages, solvers, proofs assistants | What are the properties that you are going to verify? | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/2-sdi#discussion-software-aspects-of-strategic-defense-systems",
    
    "relUrl": "/lecture-notes/2-sdi#discussion-software-aspects-of-strategic-defense-systems"
  },"22": {
    "doc": "Discussion - SDI",
    "title": "Discussion - SDI",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/2-sdi",
    
    "relUrl": "/lecture-notes/2-sdi"
  },"23": {
    "doc": "Discussion - Data Science",
    "title": "Discussion: Notebooks and literate programming",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/20-data-science#discussion-notebooks-and-literate-programming",
    
    "relUrl": "/lecture-notes/20-data-science#discussion-notebooks-and-literate-programming"
  },"24": {
    "doc": "Discussion - Data Science",
    "title": "What’s Wrong with Computational Notebooks? Pain Points, Needs, and Design Opportunities",
    "content": ". | Methodology: Field observations and exploratory interviews. Follow-up with survey . | What is the strength of the field observations? . | Do not rely on people to remember what they did, or to be able to describe it | . | Sampling strategy | What is the role of the field study in the “mixed methods” methodology? . | Exploratory - learn how people interact with notebooks in order to determine what questions to ask in the survey | “Limitations” paragraph sounds like a paragraph added in response to reviewer complaints | . | Any concerns with this methodology? . | Potential for bias from Hawthorne effect- people behave differently when studied | . | . | Pain points . | What makes these pain points specific to data science, and not just pain points for programming? Or: What about data science makes these pain points even more painful? . | Dependencies are painful in notebooks | Working with data: how to share the data and get access to it, share etc | Long-running computations - especially in the context of interactive programming, where you want to get the result quickly and then do something else based on that . | Also limited in how many tasks a developer can do concurrently with one kernel (one) | . | Reliability of the kernel - when it crashes it is bad. Especially bad when tied to something that’s long running | Tough to be both a documentation tool and a programming tool - it doesn’t do either as well as we wished. Misaligned between the tool and the use-cases . | What are those use-cases? . | Debugging: When going through a notebook, there are data-centric debugging challenges. Might be many intermediate transformations, hard to see into them to understand what is changing | Creating visualizations: How to specify what kind of figures are wanted | Specific challenges around math/statistics | Building and evaluating models: There is a lot of data that needs to be consumed to e.g. build the model, then a lot of time to compute it, then you end up with more stuff that needs to be stored (model and parameters) | | . | . | This is a means to an end for data scientists, and it’s not really a good one - jupyter notebooks is lacking what we would consider to be “good development tools” . | Lacks flexibility for tools that “traditional software engineers” need/use in iDEs? | . | . | What about pain points that are specific TO CELLS and not just to data science? . | Interactions with cells are clunky (key strokes, undo/redo) | Lack of visibility between the different cells. Copying/pasting a cell to somewhere else might have unexpected results | What the heck is the semantic definition for a cell? Is it a function? What is the functional unit? . | How long is a “readable” cell? | Comments within cells vs markdown around cells - tensions.. | . | Line numbers usually disabled by default. This works great until you get an error. But, then when you get an error, it sure would help to know the line number. Line numbers start over in each cell | Is it a problem that cells can be run in different orders? . | Some researchers feel it is a problem. Hard for reproducibility, good for exploration . | Solution: “Restart kernel, run all cells” | Just because you CAN run cells out of order doesn’t mean that you should in all cases (or maybe there is some anti-pattern, or maybe there should be some guard rails) . | When you are programming overall, there are certainly many things that you need to maintain in mental model in head, some things can get lost… | . | Another problem for reproducibility: . | We have some data files that are stored on our computer, paths might be hard-coded | . | . | How do data scientists communicate their mental model of data flow in a notebook to each other? . | Intended to be top-to-bottom as a “narrative”. Maybe not supposed to be run out-of-order? | Notebooks never intended to be reused in this kind of workflow - share and have someone else run it, then run automatically | This is a constraining requirement | Embedded execution order shows the last set of runs, but nothing about what happened before that | Shows a slice of the workflow that generated the output in the notebook (which cells in which order, but not the history of those cells or their prior execution orders) | . | . | Is there an alternative to cells? . | Immutable cells - once you run it, it becomes read only, can’t add new cells above previously executed cells | Integration with assistive technologies like 3D, virtual reality . | Organize cells in multiple dimensions | . | See recent CHI paper “sticky land” or something like this - separate the output from the code | What about constraints that limit global mutability? . | At some point it just becomes a script with extra steps - and not a notebook anymore | . | . | Pain points around versioning and sharing: . | There is a lot of metadata in the file, and that metadata is being versioned, and will cause conflicts | The output in the notebooks also goes into version control, diff’ing the output is NOT FUN. | The output might be large (especially images) | Resolving merge conflicts in notebooks? . | NBDime | . | What are the needs for versioning and sharing? . | “I am going to put this in git and version it there so that I can work on one of two machines at once” | Useful to share so that someone can digest the output of a notebook | Extract key pieces to be reused elsewhere (demo that functionality) | Also might want to share all of the extra stuff (environment settings, the data, what versions of what libraries need to be installed) | . | . | Refactoring: . | “For researchers, it’s important that we support the languages data scientists actually use, not the languages we /wish/they would use. “ - do we need languages without dynamic typing and reflection? . | Languages that run faster | Less bug-prone/provide more of a safety net (especially in terms of type systems) | . | . | . | . | . Managing Messes . | What do we think of the characterization of “messes”? What precisely is a mess? Is this a common problem? . | A “mess” is highly subjective . | Is it a mess, or exploratory programming at its finest? | . | 3 types of messes: . | “Disorder” (cells are run in a different order) . | Is it really a problem to run in different orders? Does it impact output? | . | “Deletion” (cell deleted but the content still there) . | Possibly an issue for reproducibility | . | “Dispersal” (code generating some results is spread across entire notebook, not in same area) . | This is what they address in this work | . | . | Presentation of the tool: the use case and discussion . | Program slicing . | Static | Dynamic | It is not very simple to determine a valid slice: int a = 1; int b = 2; if(a == 1){ b = 3; } print(b); //slice on this . | . | . | . | . int b = 2; int a = 1; Object foo = null; if(a == 1){ foo.bar(); b = 3; } print(b); //slice on this . | What would convince you to adopt this code gathering tool? . | Is this a problem that you’ve had with notebooks before? | Does it solve that problem? . | Maybe, but it also creates another mess: “clutter across notebooks” - code clones | What evaluation would or does demonstrate that this tool reduces code dispersal? . | Is this tool intended to prevent messes from being created in the first place, or to manage the messes? - evaluation just shows managing | How to evaluate directly: “Are the resulting notebooks less messy?” | Qualitative analysis demonstrates feasibility on a system-focused paper | . | . | . | What are interesting future directions in “literate programming”? Considering notebooks, proof assistants, whatever… . | Staging based on level of competency - provide easier paths to common solutions? | Develop more tools that are customized to the problems of notebooks. Code clones? | Do more user studies and collect more feedback to understand how these tools are used and what the challenges are . | Use this as feedback to integrate better documentation and training | . | How to link and cross-reference in notebooks? | . | . Some cool seminar on live and literate programming by Andrew Head - Syllabus: Live and Literate Programming - Google Docs, Course Schedule, Readings . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/20-data-science#whats-wrong-with-computational-notebooks-pain-points-needs-and-design-opportunities",
    
    "relUrl": "/lecture-notes/20-data-science#whats-wrong-with-computational-notebooks-pain-points-needs-and-design-opportunities"
  },"25": {
    "doc": "Discussion - Data Science",
    "title": "Discussion - Data Science",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/20-data-science",
    
    "relUrl": "/lecture-notes/20-data-science"
  },"26": {
    "doc": "Discussion - Security",
    "title": "Discussion: Why Secret Detection Tools are Not Enough &amp; Course Postmortem",
    "content": ". | Secret detection in code repositories . | What is this? . | Detect passwords that are put into code | Examples of password needed: To connect to external APIs, databases, etc | . | How do we detect secrets? . | Need to look back in history to find everything (if this happens to you, check: ohshitgit.com) | . | Why do the authors say this is important? . | The keys are put as plain-text - anybody who gets access to this repository in the future will get access to this secret | . | What do you think about this being important? . | A better fix is to have secrets that are short-lived, and hence, their disclosure is not horrible | Developers are not sufficiently focused on security - we need defense in depth . | “If this gets discovered it’s not a big deal” - This judgement should not be left to the masses. Great exploits are usually a chain of smaller exploits, and care should be taken in providing footholds | . | What about a key management system, e.g. Hashicorp vault . | Complicated | Adds friction | Many different tools, need tool-knowledge, not just conceptual knowlddge | Oh wait, you also have to decide how to configure it (who gets what keys? the social/organizational problems…) | . | Experiences with secret detection tools . | APIs that are proactively crawling GitHub looking for their keys | GitHub’s GitGuardian scanning | From the old days: the $14,000 mistake of sharing an AWS key | The scope of this problem is huge - auditing is not easy (especially finding all of them). | . | . | What are the objectives to this study? . | How often do devs bypass warnings? | What is density of checked-in secrets by-file? | What is the distribution among developers that are checking in secrets? | Why do developers ignore the warnings? | Goal: Motivate developers to NOT ignore the warnings | Goal: Make tools better to generate less false positives | “We have a budget of 10 warnings a day”… Lacework | . | What is the methodology? . | Conduct a study in XTech | How could/should this study be designed to ensure trasnferability? . | Still have humans in the loop… | Survey + qualitative findings described in context can transfer (must have sufficient context described) | . | . | Results + implications? . | How often do devs bypass warnings? . | 51.4% . | These are “potential” secrets, some may be false positives! | What about results of actual outcomes: the investigations | . | What does “bypass” mean here? . | Continue uploading to VCS regardless of warning | “One time bypass” | “Permanent bypass” | . | . | Why do developers ignore the warnings? . | Believe that there is no significant damage . | Not a production credential . | They will forget to remove it, maybe it becomes a production credential one day | Could allow an attacker lateral movement in an organization | . | No significant data exposed . | Whose decision is this to make? | . | . | It wasn’t me, I am just a refactorer . | Maybe this one is the least bad :) | Maybe you should have a policy that is: if you come across it, you have to fix it. | . | There is a difficult-to-quantify risk here: someone higher up than the dev needs to make the call to determine whether resources should be allocated to fixing this or not? | . | Why can’t secrets be promptly removed? . | Cost + training | Cost -&gt; Requires organizational buy-in to this as a priority | . | What are solutions? . | Train developers on alternatives to hard-coded secrets | Organizational support for this problem | Include this paper in software engineering security lectures | MAKE A LINTER THAT SHOWS A RED SQUIGGLE IN VSCODE THE MOMENT THAT YOU DO THIS | . | Impact/value of the survey? . | Provides clear qualitative results about the problems faced by the deployment of secret development tools | . | . | Is this a good methodology to take to evaluate all of our software engineering tools? . | Put tools in front of developers, have them use it, collect survey results as to “is this helpful” . | Helps to elicit ways to improve interface/integration with developer workflows | . | For earlier-stage experimentation, also perform observational studies | Evaluation of the tool needs to also consider the entire context of the problem - not limited to the tool or the developers - in this context, we should also measure things like security outcomes | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security#discussion-why-secret-detection-tools-are-not-enough--course-postmortem",
    
    "relUrl": "/lecture-notes/23-security#discussion-why-secret-detection-tools-are-not-enough--course-postmortem"
  },"27": {
    "doc": "Discussion - Security",
    "title": "Course Postmortem",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security#course-postmortem",
    
    "relUrl": "/lecture-notes/23-security#course-postmortem"
  },"28": {
    "doc": "Discussion - Security",
    "title": "Top papers and bottom papers poll in PollEv",
    "content": "Sometimes the papers that we were the most critical of led to the best discussions. Update poll next time to clarify top means “more time on this” and bottom means “less time on this, or not at all” . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security#top-papers-and-bottom-papers-poll-in-pollev",
    
    "relUrl": "/lecture-notes/23-security#top-papers-and-bottom-papers-poll-in-pollev"
  },"29": {
    "doc": "Discussion - Security",
    "title": "Other topics to cover",
    "content": ". | Ethics . | Discuss case studies - powerful examples to motivate further discussion | Value sensitive design? | Find papers/studies that involve cultures that are typically not studied in research literature | . | ML/AI - MLops, SE for AI, AI for SE, LLMs, etc . | Background ethics discussion | GitHub Copilot + generally LLMs of code | Testing + AI . | Testing LLMs | Using LLMs for testing | . | . | DevOps overall . | What happens when you start deploying the product? | SLA, uptime guarantees | Disaster recovery | More infrastructure as code | . | Reproducibility + maintainability in experimental software | Including discussions of social justice, equity and diversity throughout the class, not just in a single meeting | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security#other-topics-to-cover",
    
    "relUrl": "/lecture-notes/23-security#other-topics-to-cover"
  },"30": {
    "doc": "Discussion - Security",
    "title": "Class format/participation etc",
    "content": ". | Lukewarm on the idea of having a weekly deliverable | Students sign up to present papers . | A particularly good approach for focusing on a specific research interest | . | Reflection paper? . | Provide more precise specification, especially about the degree to which each paper needs to be covered | Drop the requirement for papers from the class (still strong suggestion on topics) - but can still use a paper from the class | Add a paper swap/peer review | . | “Advanced Software Engineering II”? | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security#class-formatparticipation-etc",
    
    "relUrl": "/lecture-notes/23-security#class-formatparticipation-etc"
  },"31": {
    "doc": "Discussion - Security",
    "title": "Discussion - Security",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/23-security",
    
    "relUrl": "/lecture-notes/23-security"
  },"32": {
    "doc": "Discussion - Design",
    "title": "Discussion: The Quality Without a Name and Reuse/Compression",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/3-design#discussion-the-quality-without-a-name-and-reusecompression",
    
    "relUrl": "/lecture-notes/3-design#discussion-the-quality-without-a-name-and-reusecompression"
  },"33": {
    "doc": "Discussion - Design",
    "title": "The quality without a name, and software",
    "content": ". | How does RPG/Christopher Alexander describe this? . | “Alive” - something that is updated and grow/be updated. In OSS: something where contributors can come along, open a PR, and make a contribution. The artifact itself has needs and requires care and artfulness. Lifecycle. Unique | It is only the “whole” when it is considered with the greater part of the system, which includes all of the future pieces, and all of the human pieces - the | . | How would you? . | Is it “Elegance” - integration, beauty with functionality? . | Elegance is in the eye of the beholder, “maximize depth (selected quality attributes) while minimizing complexity” | . | Is it “usability” (in the sense of usable by the developers, not neceassarily by the users) - necessary, but not sufficient. Maybe this is too neutral of a word (need to also consider habitability and overall happiness) | Trustworthiness - how this operates and provides value to humans | . | Have you worked with software in a way that resembles this? . | We have worked with codebases where it DEFINITELY DOES NOT, and perhaps some where it is better… | . | How do we make software in this manner? | Is it possible to objectively define “good design” in software? . | …Or is it “unobtainium” (something to strive for, we can always get closer to) | These words are not meant to be definitions, they require contextual and cultural analysis | Lack of internal contradictions | Maybe this is the wrong goal - or at least a very hard job | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/3-design#the-quality-without-a-name-and-software",
    
    "relUrl": "/lecture-notes/3-design#the-quality-without-a-name-and-software"
  },"34": {
    "doc": "Discussion - Design",
    "title": "Reuse and Compression",
    "content": ". | In what ways /can/ we reuse code? Which ways does RPG miss? . | One way: copy/paste. This does not “compress” | Use packages (e.g. system packages, libraries, components) | Use standard interfaces (e.g. POSIX) - Get the INTERFACE right (this is a very big risk, separate from getting the implementation right) | Algorithmic abstractions - generic algorithms that can be applied to many instances | Reuse requirements (including the lower-level specifications). Example: automotive/self-driving cars. | “We got it right once, reuse it” (for requirements, for code) - assuming that it has! . | What is the incentive for the person that makes the library? It is hard to make the “perfect” code, and there are more risks to those bugs if others come in and depend on our code | . | . | Why compare reuse and compression? . | Code is more dense - could be good or bad, or both… Utilizes context, and there is some burden on the developer trying to understand it to understand that context | If you understand the library that you are reusing, then compression will help you understand more about the application | Law of leaky abstractions: all abstractions are leaky (not perfect - leaks the underlying complexity) | The more you write code, it can become denser, good for adding new things, but need to know the context… or hard, because it’s complicated to read/understand when you come in to change things | RPG: locality is important, so that someone who comes along and needs to read/understand, can find that context | Compression is a way to describe reuse-like property of OOP | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/3-design#reuse-and-compression",
    
    "relUrl": "/lecture-notes/3-design#reuse-and-compression"
  },"35": {
    "doc": "Discussion - Design",
    "title": "Discussion - Design",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/3-design",
    
    "relUrl": "/lecture-notes/3-design"
  },"36": {
    "doc": "Discussion - AOP and RP",
    "title": "Discussion: AOP and RP Evaluation",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/4-aop-rp#discussion-aop-and-rp-evaluation",
    
    "relUrl": "/lecture-notes/4-aop-rp#discussion-aop-and-rp-evaluation"
  },"37": {
    "doc": "Discussion - AOP and RP",
    "title": "Aspect-Oriented Programming",
    "content": "The hot takes . | Is it annoying to have to have an additional compilation/steps? And also, to have to learn new language(s)? | Is this both easy to understand and efficient? . | How do you actually evaluate the learnability of this, and the understandability of the resulting code? | . | Is HTML/CSS a similar kind of “cross-cutting” concern . | (In the context of setting styling for multiple objects) | . | How does this actually work? Can you do it in interpreted language? | This is a very different way to design programs, how to come up with new ways (like this and other?) | Are these “just macros”? | Are there some contexts where this works the best? | How do you debug this!? | Is it a good thing or a bad thing that the compiler is not “smart”? . | Separating the compilation of the program from the aspects might help with reuse across different languages | . | . Aspects . | Why? . | Coupling (Want low coupling) - Degree of interdependence between program units. Change in one unit requires change in many others (perhaps also hard to figure out which) | Cohesion (Want high cohesion) - Degree to which a program unit represents a single purpose | Defined in terms of “program unit” . | Functional | Now: aspects, “cross-cutting concerns” to reduce “tangling” (when you try to do more than one thing at once) | What is tangling? . | Subjective, but it can be more precise based on stated quality attribute goals | Perception of how complicated it is to determine “what is doing what” | . | . | . | The example in section 3: Pixelwise . | Loop fusion | Is this easier to understand? . | Fewer lines of code != easier to understand | . | Is this easier to maintain? . | Reducing copy/pasted code might help | This probably DOES make it much easier to maintain PROVIDED that the developers doing that maintenance fully understand it. | It can be hard to reason about what gets called when, if an aspect join point occurs - you won’t know upfront that it will be called/included. Hard to isolate things. Leaky abstraction | “” sounds good | . | . | There are additional constraints to what kinds of things you can call “aspects” and how to program around | . | Digital library example . | Aspect: communication - avoid sending unnecessary information | Are there other alternatives? . | Refactor Repository interface, method unregister(book) becomes unregister(isbn) | Make a subclass of book, called RemoteBook . | Challenge to both this and the aspect approach: what do you do when someone tries to access other properties? | . | Add the transient modifier to the other fields | . | Creating these aspects requires a strong understanding of the functionality that you are implementing and need to get to | Here is a next-level aspect: only copy what is needed, automatically | . | How would/could you evaluate this? . | User study - examine code, answer questions about it | Ask users to debug some code . | How to design a controlled debugging experiment? | . | Another user-study idea: provide the participants with fragments of the solution ,and fragments that do not belong in the solution, and then asking users to arrange them (“parsons problem”) | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/4-aop-rp#aspect-oriented-programming",
    
    "relUrl": "/lecture-notes/4-aop-rp#aspect-oriented-programming"
  },"38": {
    "doc": "Discussion - AOP and RP",
    "title": "An empirical study on program comprehension with reactive programming",
    "content": ". | Comparing the two examples in figure 1: . | In the observer side, the dependencies are inverted: the “update c” logic has to be attached to a. | If there are a lot of dependencies inside of the variable, this problem cascades to become much worse | . | To find the dependencies, you need to find all of the add observer calls, in the RP version, it is in one spot (where the signal is) | . | Does anyone have RP experiment, and if so, with what? . | React . | If you use only the RP parts, it’s good | React has asynchronous updates, which is a separate problem/mess | . | ShinyR | Hardware description languages | . | Evaluation methodology: . | Controlled experiment, measuring the time to complete tasks and and success rate, plus some qualitative feedback | What are the tasks? . | “What do you expect this output to be?” | Some tasks were excluded based on being too straightforward in one case | Behavioral understanding, which require tracing the functional dependencies | What tasks might be harder in RP? . | Tracing relationships between multiple parameters, this focuses mostly on dataflow | Know the order of when things happen | . | . | What was the training? . | 2 class sections on RP, 2 homework assignments on RP | Prior course experience on OOP, maybe from first year of studies (these are 4th year students). Observer pattern was learned in first year | . | They built a custom tool for running the user study | Findings: . | RP is better | We liked the statistical tests | Liked the findings that were not statistically significant | . | Discussion/concerns . | How do the goals of the tasks relate to the user interaction with the study and its training | Fairly impartial, not concerned about overlap of authors | The training quality is likely to be MUCH better than could be expected broadly. It would be very revealing to show the lectures and assignments, to see overlap | Having and reporting on a pre-test would be nice here, too (what was maximum level of exposure) | Showing the specific tasks in a table to line up with the results would help with further understanding/analysis | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/4-aop-rp#an-empirical-study-on-program-comprehension-with-reactive-programming",
    
    "relUrl": "/lecture-notes/4-aop-rp#an-empirical-study-on-program-comprehension-with-reactive-programming"
  },"39": {
    "doc": "Discussion - AOP and RP",
    "title": "Discussion - AOP and RP",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/4-aop-rp",
    
    "relUrl": "/lecture-notes/4-aop-rp"
  },"40": {
    "doc": "Discussion - OSS",
    "title": "Discussion: Open Source",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . | Review course requirements: . | Individual Reflection Paper (proposal with reading list: Feb 7, ~2500 word final document: Mar 14, Mar 14 &amp; 16 presentations) | Group Project (Feb 23 preliminary proposal, Mar 2 final proposal, Mar 28 status update, Apr 18 report, Apr 18 &amp; 20 presentations) | In-class participation (attendance, completing polls, participating). | . | Review course grading: . | Each of the above 3 aspects gets an overall grade: Check-, Check, Check+ | Paper and project each have extremely detailed rubrics | Participation: Can miss up to two weeks with no penalty | Earn at least check on all 3: A | Earn at least check on 2: B | Earn at least check on 1: C | +/- are discretionary | All grades manually computed at end of semester, do not rely on Canvas averages | . | Poll and high-level discussion: OSS . | Poll will ask for evaluation on two dimensions: https://pollev.com/jbell . | Significance: Do you think that there is a practical impact of this study? (We will discuss those impacts) | Soundness: Do you think that the methods are appropriate, and that the results support the conclusions that are drawn? | . | Forking paper: . | Significance . | Interviews on collaboration interesting and can have relevance | Avoiding hard forks is interesting, but not the main focus of the article | Good contribution for learning about forking, history, implications, etc. | There has been a significant increase in forking (good), increase in OSS, collaboration (good), increase in hard forks (?) | . | Soundness . | How to interpret response rate? What conclusions to draw? | Heuristics-based classifiers for detecting social forks vs hard forks, hard to tell exactly what the heuristics optimize for, whether it is even possible to make a clear classification | . | . | Open source donations . | Significance . | Selection of platforms (can’t make claims beyond what they studied) | Interesting conclusion that there is not a significant correlation between level of activity and donations | What is the right question to ask here? . | Over what time period to consider activity? | . | . | Soundness . | Are there strong confounds: projects only ask for money once they are already in a relatively mature phase? | Using only public artifacts vs interviews/survey | RQ5: How was the money spent? | Compares across domains | Volume of donations, value of donations, amount activity really varies by project AND by domain | . | . | . | Broader OSS discussion/community issues . | What are the expectations of community members . | Contributors . | Contributions to be accepted? | . | Users . | Bug-free? | . | . | . | Forking . | There is a spectrum between hard forks and social forks | “We started as a social fork, then it became a hard fork” | “Significant increase in forks” - but short-lived . | How to make forks sustainable? | . | What are our own experiences about making a fork? . | Merging back upstream? | Intentional not to merge upstream: a feature just for us | “Successful PR” - make the fork, then merge upstream | “I just had to make a fork of the class project” | . | What are the motivations for project maintainers to NOT merging in a PR? . | Supply chain attacks | Maintenance burden | Vanity [or maybe something is intentionally snapshotted] | It is not a feature that we want | Licensing conflicts | . | What are the motivations for project maintainers TO merge them in? . | Diversity the volunteer pool, bring in new talent | Free bug fixes | What about a “disruptive innovation?” . | Creativity, community contribution | Build on top of existing projects to make something new/bigger | . | Forks can increase the utilization of the overall project . | That fork might have its own pool of users, contributors, etc. WebRTC | . | . | Methodology: . | Is there a more objective way to determine hard forks, OR: is it OK, OR: should we change the question to be easier to answer? . | Maybe this should be a non-discrete (Not hard/social)? . | Based on a metric based on the flow of changes in both directions | . | Maybe the study is interesting without the “hard fork” part? . | (“We found that the stigma around hard forks is mostly gone”) | . | . | . | Results: . | “Hard forks are not likely to be avoidable” | Community fragmentation is problematic… could we measure that directly? . | Forks, new projects, projects that just don’t happen | . | What about community/governance models? . | It is organic. | What is GitHub’s role in governance? (Nudging) | BDFL vs hierarchy | . | . | . | Donations: . | Did the decision of the authors of the donations to not interview developers cause them to miss out on significant information? . | This is a difficult topic to discuss and get answers on | This is a possible topic to get valid/good answers on, but it is hard and time consuming to do right | . | Other soundness complaints: . | Bitcoin | Projects supported by a foundation? | Other support mechanisms (paid version vs free version) | . | Other significance complaints: . | We really want to compare self-run donations vs other funding sources | “How are they spending the money” - this is a philanthropy-rooted question, maybe turn to that research | What is the level of funding that is needed to sustain a project? . | Bringing funding in to a project may have unexpected outcomes (again, look to philanthropy research) | . | . | Methodology . | Is this “look at open source projects” and then follow-up with interviews a good methodology overall? . | Good, but hard to draw general conclusions | Can be very limiting based on the selection of data sources. | OK with just using GitHub? | OK with just using OpenCollective? | . | There are non-open-source collaboratively funded projects that are excluded from such a study (e.g. games) | . | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/6-oss#discussion-open-source",
    
    "relUrl": "/lecture-notes/6-oss#discussion-open-source"
  },"41": {
    "doc": "Discussion - OSS",
    "title": "Discussion - OSS",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/6-oss",
    
    "relUrl": "/lecture-notes/6-oss"
  },"42": {
    "doc": "Discussion - MSR",
    "title": "Discussion: Mining Software Repositories",
    "content": "These notes roughly capture the (mostly unedited) comments from the class: . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/8-msr#discussion-mining-software-repositories",
    
    "relUrl": "/lecture-notes/8-msr#discussion-mining-software-repositories"
  },"43": {
    "doc": "Discussion - MSR",
    "title": "General discussion",
    "content": "When is a study “exploratory” vs “definitive?” . | Even if you say “exploratory” people still jump to conclusions | Depends on the context, maybe | Transparency in data/results/study can help future work to confirm/refute/clarify | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/8-msr#general-discussion",
    
    "relUrl": "/lecture-notes/8-msr#general-discussion"
  },"44": {
    "doc": "Discussion - MSR",
    "title": "Discussion: “The Promises and Perils of Mining GitHub”",
    "content": "What is wrong with using GitHub for personal projects, wrt perils? . | Heavily dependent on what you are trying to study - “well-engineered” vs… personal . | Not “wrong” to use GH for personal projects, but maybe bad to assume that they are well-engineered. In fact: having a dataset of these “less engineered” projects could be useful for some applications | “Contaminates data” - across categories | . | The purpose of GitHub is not to provide a database for mining :) | There are a variety of ways to “sanitize” the data: Committers, commit frequency, recency, presence of forks/pull-requests/issues | . Perils of linking issues with code: Biased samples based on population, bugs in bug trackers aren’t all bugs, commercial projects != open source projects, large sample size “might” help . GHTorrent - an interesting resource . Study design . | 1,000 GH users get an open-ended survey, 240 responses . | relatively small, biased to those who had published their emails | would bigger survey result in different conclusions? | reach saturation? | . | Qualitative analysis of metadata | Manual analysis for 434 projects | . The perils . | Repository != project . | Forks | What about those 11k commits in forks not merged into upstream? | . | Activity is measured in commits . | Most projects have very few commits | . | Most projects inactive . | What is “active” - commit in past 6? months | Should we measure activity in terms of users? (How to measure usage, how to correct for popularity) . | Note that “inactive” != “dead” - maybe important to distinguish “used” and “active” and “alive” | . | Most active 2.5% of projects account for remaining 97.5% of projects | . | Large portion of repos not for software development . | used just for storage. Configuration files, resumes, datasets, etc. | not “well engineered projects” but something else | Static websites | “Assets” other kinds of blobs, etc. | Books (“books as code”) | . | 2/3 repos personal . | 90/240 respondents said they use GH mostly for personal projects and not with intention to communicate | Maybe connected to the response rate/bias here - who posts email/responds/has social projects? | Followed-up based on quantitative analysis of how many commiters there are . | There are personal projects that have multiple fly-by committers | There are significant community projects who primarily have single committers | . | . | PRs are valuable, but not used everywhere | PRs can be reworked and lose discussion | Most PRs appear non-merged even though merged . | Could this be a UI problem, now fixed? | Can lead to inaccurate data for studies that look at what drives “success” in merges | Bot PRs | . | Many active projects don’t use GitHub exclusively . | Mirrors | Hard to expect everything in one place | . | . The promises . | PRs are valuable source of data | Linking developers, PRs, issues and commits | . Things that are absent: . | Mining the CODE itself | Repository templates - where does this fit? . | Forks that you cannot tell are forks | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/8-msr#discussion-the-promises-and-perils-of-mining-github",
    
    "relUrl": "/lecture-notes/8-msr#discussion-the-promises-and-perils-of-mining-github"
  },"45": {
    "doc": "Discussion - MSR",
    "title": "Discussion: “A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts.”",
    "content": ". | Motivation/intro . | Jet brains research | . | What are interesting questions to ask of large-scale notebook datasets? . | Is code in notebooks different? (Procedural vs exploratory) | Is the code written for different reasons? | How many users have both (we can link committers!) | How often are notebooks used collaboratively vs individually? | What are the most popular languages that are used for notebooks? | Are notebooks “actively developed” in the same way of non-notebooks | Who is using these tools, and why? (Students, domain scientists,…) | Is the style of the code different depending on the context? | Why migrate to and away from notebooks? | Examine which libraries are used in the notebooks, cluster usage, popularity, etc | . | Methodology . | Metrics . | Structural metrics . | Number of built-in functions, user-defined functions (definitions? usages?), API functions | Cyclomatic complexity - proxy measure for understandability. (Maybe some confounding things) | Number of imported functions | Cell coupling . | How many variables are referenced between cells | . | Extended comment LOC . | Comments + markdown lines | . | . | Stylistic metrics . | Linters | Only ran linters on 100k files | How slow was this to run? Why was that the case? | . | . | Datasets . | All jupyter notebooks in Nov 2020 (9.7m) excluding in fork repos . | Took 1 month | 1.7m projects | Only python notebooks: 8.3m | Filtered to only those with “permissive” license: 847,000 . | “Copyleft” - if you change the code and distribute the compiled code you must give away your changes to source | “Permissive” - do whatever you want | . | Sampling? . | They included all that met these criteria? | . | . | Compare notebooks to non-notebooks: . | 10k most-starred projects on GH with permissive licenses | Sampling? . | Treated every single file separately, didn’t weight per-repo | What KINDS of projects are these? | . | . | . | Results . | “Has no effect” linting rule is not helpful, it occurs widely | “Uses the same variable name” repeated IS something where there might be a helpful intervention | Jupyter notebooks “are more error-prone” . | Were the non-notebook files listed normally? | Also has to do with the use cases | . | Function usage and complexity? . | Far fewer functions in notebooks than scripts | More usage of functions/APIs in notebooks than scripts | Implications? . | Different programming models wrt structure | . | . | Notebooks were longer . | Notebooks might “drag” on and on [let’s look at evolution!] | Diff’ing notebooks :’( | . | Are there raw numbers? . | Invalid statistical tests | . | . | . | . ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/8-msr#discussion-a-large-scale-comparison-of-python-code-in-jupyter-notebooks-and-scripts",
    
    "relUrl": "/lecture-notes/8-msr#discussion-a-large-scale-comparison-of-python-code-in-jupyter-notebooks-and-scripts"
  },"46": {
    "doc": "Discussion - MSR",
    "title": "Discussion - MSR",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/lecture-notes/8-msr",
    
    "relUrl": "/lecture-notes/8-msr"
  },"47": {
    "doc": "Announcements",
    "title": "Announcements",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"48": {
    "doc": "Announcements",
    "title": "Project showcase posted",
    "content": "Apr 30 &middot; 0 min read The project showcase is now live, hosting posters summarizing student projects from Spring 2023. ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"49": {
    "doc": "Announcements",
    "title": "Project poster deliverable",
    "content": "Apr 11 &middot; 0 min read Please note that the final project deliverable specification now includes a discussion of the “project poster” and presentation. ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"50": {
    "doc": "Announcements",
    "title": "Schedule updates",
    "content": "Mar 22 &middot; 0 min read Please note that the schedule for weeks 13 and 14 have been updated, and “Security” will now be our final topic (keeping with the fine tradition of security being an afterthought…). Please also find the complete list of articles selected by students for their reflection papers on the reflection paper assignment page. ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"51": {
    "doc": "Announcements",
    "title": "Course &amp; Registration Information",
    "content": "Jan 2 &middot; 0 min read Welcome! A preliminary reading list is now posted on the course calendar. The exact due dates and structure of assignments is likely to change in the coming days, to be finalized before the first day of class. ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"52": {
    "doc": "Announcements",
    "title": "Course &amp; Registration Information",
    "content": "Oct 29 &middot; 0 min read Welcome! I am planning to post a complete reading list for this seminar over the next few months (by December). If there are any particular topics that you are interested in, please feel free to email me, and I’ll see about fitting them in. In the meantime, please see the syllabus posted on this website for more information. ",
    "url": "/CS4910-7580-Spring-2023/announcements/",
    
    "relUrl": "/announcements/"
  },"53": {
    "doc": "Assessments",
    "title": "Assessment Overview",
    "content": "A summary of the three mechanisms that will be used to assess your performance in the course is below. Please note the course grading policy, which describes how these three assignments will be used to determine your final course grade. ",
    "url": "/CS4910-7580-Spring-2023/assessments/#assessment-overview",
    
    "relUrl": "/assessments/#assessment-overview"
  },"54": {
    "doc": "Assessments",
    "title": "Participation",
    "content": "This class is effectively a research seminar, and it is the most fun and rewarding when everyone comes to each class meeting prepared to discuss the assigned paper. I ask that you demonstrate your preparation by being ready to engage in discussion. You need not have complete mastery of the paper that we are discussing — but having a comment to offer or question to ask is always appreciated. If you do not feel comfortable participating in class, please reach out to me — I would like to create an environment that is welcoming for discussion, and would not like to inadvertently penalize a student who struggled to find a moment to speak up in class. As an alternative, you may also (or instead) post on the class Discord a brief (3-5 sentence) reaction to the papers that we will be discussing in class, and engage in discussion through text. I will provide you with weekly feedback on your participation in class, at the level of “check”, “check -“ and “check +.” If you receive a “check -“, you may replace that week’s grade with a check by coming to my office hours (or scheduling a time outside of office hours) to have a brief discussion about your thoughts on that week’s papers. Your final participation grade will be calculated as follows: . | Receive a Check on participation if at least 12/14 Check’s or Check+’s throughout the semester. | Receive a Check - on participation if at least 12/14 Check-‘s, Check’s or Check+’s throughout the semester. | . Note that this policy allows you to entirely miss up to two weeks of participation, with no penalty on your final grade. Remote Participation . If you are registered for an on-the-ground section are not able to attend in person (for any reason), please contact your instructor at least 12 hours in advance of the scheduled class (no need to share a specific reason or photo of a COVID test). Please, note that we are not well-equipped to offer a symmetrical experience for remote attendance and in-person attendance, and do not consider joining remotely as a perfect substitute, but rather an improvement over missing class entirely. No student should expect to have this accomodation granted more than three times during the semester. ",
    "url": "/CS4910-7580-Spring-2023/assessments/#participation",
    
    "relUrl": "/assessments/#participation"
  },"55": {
    "doc": "Assessments",
    "title": "Reflection Paper",
    "content": "The goal of the reflection paper is for you to select a topic of particular personal interest, identify four research articles (that are not required readings for this course), read them, and organize your reaction to the topic in a reflection paper. The reflection paper provides you to an opportunity to apply critical reasoning skills in the context of the research papers that we discuss in class, and to dig deeper into a topic of your own particular interest. The expected length of the reflection paper is roughly 2,500 words. The paper will be accompanied by a presentation. Reflection Paper Details and Rubric . Intermediate deliverables: . | Reflection Paper Proposal | . ",
    "url": "/CS4910-7580-Spring-2023/assessments/#reflection-paper",
    
    "relUrl": "/assessments/#reflection-paper"
  },"56": {
    "doc": "Assessments",
    "title": "Implementation Project",
    "content": "The project will involve a hands-on application of the techniques and tools that we discuss in class, and can be completed either individually or in a group of at most two. Topics for the project will be discussed in the first seven weeks of class, and your specific project topic will be finalized soon after. Both research-oriented projects (which implement and evaluate some new idea) and engineering-oriented projects (which make contributions to tools that developers may already use) are welcomed. Implementation Project Details and Rubric . Other deliverables: . | Preliminary Project Proposal | Revised Project Proposal | Project Status Update | . ",
    "url": "/CS4910-7580-Spring-2023/assessments/#implementation-project",
    
    "relUrl": "/assessments/#implementation-project"
  },"57": {
    "doc": "Assessments",
    "title": "Assessments",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/assessments/",
    
    "relUrl": "/assessments/"
  },"58": {
    "doc": "Calendar",
    "title": "Calendar",
    "content": "Class meetings marked as “overview” will be lecture-focused, providing background material to help contextualize the topic. “Discussion” meetings will be highly interactive discussions centered on the required readings. Please be sure to read the assigned paper before class. Most papers link into the ACM or IEEE library - you can sign in to those services using your Northeastern login (select “Sign in with institutional credentials” and then select Northeastern). Note that for the ACM library, you can create an ACM account and then link it to your Northeastern ID (“My Profile” -&gt; “Institutional Affiliations”) so that you can stay logged in and not need to go through Duo every time that you would like to read a paper. ",
    "url": "/CS4910-7580-Spring-2023/calendar/",
    
    "relUrl": "/calendar/"
  },"59": {
    "doc": "Home",
    "title": "Advanced Software Engineering",
    "content": "CS 4973/7580, Spring 2023, Tues 11:45-1:25pm/Thurs 2:50-4:30pm, Richards Hall 325 . ",
    "url": "/CS4910-7580-Spring-2023/#advanced-software-engineering",
    
    "relUrl": "/#advanced-software-engineering"
  },"60": {
    "doc": "Home",
    "title": "Project showcase posted",
    "content": "Apr 30 &middot; 0 min read The project showcase is now live, hosting posters summarizing student projects from Spring 2023. Announcements . ",
    "url": "/CS4910-7580-Spring-2023/",
    
    "relUrl": "/"
  },"61": {
    "doc": "Home",
    "title": "Instructor",
    "content": "Jonathan Bell . j.bell@northeastern.edu . Office Hours: Available most days 9am-5pm on Discord, or by appointment via email . ",
    "url": "/CS4910-7580-Spring-2023/#instructor",
    
    "relUrl": "/#instructor"
  },"62": {
    "doc": "Home",
    "title": "Teaching Assistants",
    "content": "Katherine Hough . Office Hours: TBD . ",
    "url": "/CS4910-7580-Spring-2023/#teaching-assistants",
    
    "relUrl": "/#teaching-assistants"
  },"63": {
    "doc": "Home",
    "title": "Overview",
    "content": "Building, delivering and maintaining successful software products requires more than being good at programming. Software engineering encompasses the tools and processes that we use to design, construct and maintain programs over time. Software engineering has been said to consider the “multi person development of multi version programs.” Development processes that work well for a single developer do not scale to large or even medium-sized teams. Similarly, development processes that work well for quickly delivering a one-off program to a client cause chaos when applied to a codebase that needs to be maintained and updated over months and years. This class will explore recent research in software engineering, focusing particularly on automated approaches that help developers create higher quality software, faster. The primary objective for this course is for students to learn about cutting edge processes and techniques for engineering software. Topics will include: testing, agile processes, continuous integration, open source ecosystems, end-user programming, and security. We will study research papers that apply a variety of methods for studying software engineering, including studies of open source software and also observational studies and interviews of developers. The course delivery will be a mixture of lectures and discussions of research articles. ",
    "url": "/CS4910-7580-Spring-2023/#overview",
    
    "relUrl": "/#overview"
  },"64": {
    "doc": "Home",
    "title": "Pre-requisites",
    "content": "Students are expected to have experience engineering software beyond the scope of programming assignments in courses. Examples might include software written for research projects, for personal projects, or for a product. There is no requirement to be able to program in any particular language, but working knowledge of Java, C, TypeScript, Ruby, Python or R will be useful for the implementaiton project. ",
    "url": "/CS4910-7580-Spring-2023/#pre-requisites",
    
    "relUrl": "/#pre-requisites"
  },"65": {
    "doc": "Home",
    "title": "Intended Audience",
    "content": "This course is intended for undergraduate and graduate students who would like to prepare for research in software engineering and other adjacent fields of computer science (for example: security and systems). This course is also designed to be highly applicable to students who are not interested in pursuing a career in research, but who would like to become more productive software engineers in industry. ",
    "url": "/CS4910-7580-Spring-2023/#intended-audience",
    
    "relUrl": "/#intended-audience"
  },"66": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/",
    
    "relUrl": "/"
  },"67": {
    "doc": "Reflection Paper Proposal",
    "title": "Reflection Paper Proposal",
    "content": "The structure of this class is designed to provide a broad survey of many topics in software engineering practice and research: . | Software Process | Modularity and Design | Mining Software Repositories | Open Source | Testing | Continuous Integration | Devops | Expertise and knowledge sharing | Security | Software engineering in specific domains | . The goal of the reflection paper is for you to select a topic of particular personal interest, identify five research articles (that are not required readings for this course), read them, and organize your reaction to the topic in a reflection paper. The reflection paper provides you to an opportunity to apply critical reasoning skills in the context of the research papers that we discuss in class, and to dig deeper into a topic of your own particular interest. This is not a literature review: the goal with this paper is for you to provide some editorial critiques of the work and express your own opinions, rather than to provide a fair and direct summary of the contents of each paper. The expected length of the reflection paper is roughly 2,500 words. Before you begin writing (or reading in depth), you will submit a paper proposal, indicating the topic area that you intend to dive into, one required reading from the course that you intend to include, and the five additional readings in this topic that you intend to read. The articles that you select for your reading list should be selected from top-tier research venues for the selected topic. We will provide you with feedback on your reading list. Examples of top-tier venues include: . | Software Engineering (broadly construed) - ACM/IEEE International Conference on Software Engineering (ICSE), ACM Conference on Automated Software Engineering (ASE), ACM/ESEC Conference on Foundations of Software Engineering (FSE), IEEE Transactions on Software Engineering, ACM Transactions on Software Engineering and Methodology | Human Aspects - ACM Conference on Human Factors in Computing Systems (CHI), IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) | . This is not an exhaustive list, but may be a good place to start looking for articles of interest - you might look at recent articles published in these venues, or search on the ACM digital library for articles in your topic of interest, with an eye towards these venues. Submit a text document with the following specifications: . | State the topic that you are focusing on (selected from the list of topics above) | Provide 2-3 sentences describing your motivation for interest in that topic | Identify the article from the course reading list that you will include in your reflection paper | Identify the five additional articles that you will include in your reflection paper. Provide a citation (title, authors, and venue) and a link to the article | . A good-faith, on-time proposal that meets the specifications above is necessary to receive a “check” on the paper overall. This is an individual assignment. The goal of this assignment is to provide your personal reflection to the topic of your choice. Papers that are substantially similar to each other (or to third-party content) will receive a score of 0, with no opportunity for a revision. If you have any questions or concerns of what constitutes plagiarism, please reach out to the instructor immediately. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper-proposal/",
    
    "relUrl": "/assessments/paper-proposal/"
  },"68": {
    "doc": "Reflection Paper",
    "title": "Formatting",
    "content": "Feel free to use whatever word processing environment you prefer. We strongly encourage that you use some editor that will help you manage a bibliography — like LaTeX + BibTex, or Word + Endnote. Please clearly identify the one paper from the required reading list for the course plus the five additional papers that you intend to discuss — one way to do that might be to include them in your bibliography, and cite them in the first paragraph of your paper. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#formatting",
    
    "relUrl": "/assessments/paper/#formatting"
  },"69": {
    "doc": "Reflection Paper",
    "title": "Presentation",
    "content": "To share your experience with the rest of the class, prepare a lightning talk (8 minutes) that summarizes what you learned about this topic, and your reaction to it. The goal of the lightning talk is to share one or two things that you learned that were interesting to you: an open (or closed) problem in that research area, some methodology or tool that addresses a known problem, etc. The presentation will be graded entirely on a “you did it” or “you didn’t” basis, as long as you show up at the assigned time and talk about the topic. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#presentation",
    
    "relUrl": "/assessments/paper/#presentation"
  },"70": {
    "doc": "Reflection Paper",
    "title": "Grading",
    "content": "Your reflection paper will be graded on the scale of (Unacceptable, Check-, Check, Check+). The criteria for each grade are described below. To receive the grade of Check, the paper/presentation must satisfy all of these criteria: . | The paper is approximately 2,500 words (plus or minus a few hundred is OK; if you are far short of 2,500 you might consider deepening your exploration; if you are far over 2,500 words, you might consider condensing your efforts) | The paper responds to one of the articles on the course’s required reading list, plus the five articles that were approved by the course staff via the paper proposal | The paper demonstrates a thoughtful understanding of the papers selected | The paper uses relevant examples from the papers discussed in class to support and describe your reactions to the works | The paper uses language that is understandable, and generally does not distract from the content of the work | The presentation describes a concept related to the paper. | . To receive the grade of Check-, the paper must satisfy all of these criteria: . | The paper is between 1,500-2,500 words | The paper responds to one of the articles on the course’s required reading list, plus the five articles that were approved by the course staff via the paper proposal | The paper demonstrates a limited understanding of the papers selected, focusing primarily on summarizing the content of the papers without demonstration of higher-level understanding of them | . Submissions that do not meet the criteria for “Check-“ will receive the grade of “Unacceptable.” Submissions that exceed the qualities outlined for “Check” (e.g. include additional, insightful connections between the papers discussed in class and other works, uses particularly sophisticated and engaging language, and generally demonstrates an exceptional and thorough understanding of the selected papers) may receive the grade of “Check+”. As per the course policies, note that a grade of “Check” is sufficient to receive an A in the class. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#grading",
    
    "relUrl": "/assessments/paper/#grading"
  },"71": {
    "doc": "Reflection Paper",
    "title": "Submission",
    "content": "Submit your paper on Canvas. The paper is due on March 14 at 11:00am. No late submissions will be accepted. Presentations will occur in-class on March 14 and 16. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#submission",
    
    "relUrl": "/assessments/paper/#submission"
  },"72": {
    "doc": "Reflection Paper",
    "title": "Revision",
    "content": "Students who make a good faith effort on the paper (e.g. submit a document on time that is conformant to the general guidelines explained above), and receive a grade below “Check” will receive detailed feedback on their paper, and be allowed to resubmit the paper (within 2 weeks), under the understanding that the highest grade of the submissions will be used. Students who receive a grade of “Check” may not resubmit their work to attempt to achieve a “Check+” (again, note that in this situation, this would have no impact on your course grade). ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#revision",
    
    "relUrl": "/assessments/paper/#revision"
  },"73": {
    "doc": "Reflection Paper",
    "title": "List of all articles selected by students",
    "content": "We have aggregated all of the articlds selected by students for their reflection papers into lists organized by topic (topics with more articles listed reflect topics that more students selected for their reflection papers) . Modularity and design . | “Comparative analysis of functional and object-oriented programming”, Alic et al. | “A comparative study of programming languages in rosetta code”, Nanz and Furia | “A large-scale study of programming languages and code quality in github”, Ray et al. | “On the impact of programming languages on code quality”, Berger et al. | “Code quality metrics for the functional side of the object-oriented language c#”, Zuilhof et al. | K Fedoseev, N Askarbekuly, A E Uzbekova, and M Mazzara. Application of data-oriented design in game development. Journal of Physics: Conference Series, 1694(1):012035, dec 2020. | Apostolos Ampatzoglou and Alexander Chatzigeorgiou. Evaluation of object-oriented design patterns in game development. Information and Software Technology, 49(5):445–454, 2007. | Weishan Zhang, Dong Han, Thomas Kunz, and Klaus Marius Hansen. Mobile game development: Object-orientation or not. In 31st Annual International Computer Software and Applications Conference (COMPSAC 2007), volume 1, pages 601–608, 2007. | David Wingqvist, Filip Wickström, and Suejb Memeti. Evaluating the performance of object-oriented and data-oriented design with multi-threading in game development. In 2022 IEEE Games, Entertainment, Media Conference (GEM), pages 1–6, 2022. | Jessica D. Bayliss. The data-oriented design process for game development. Computer, 55(5):31–38, 2022. | Jessica D. Bayliss. Developing games with data-oriented design. In Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation, GAS ’22, page 30–36, New York, NY, USA, 2022. Association for Computing Machinery. | . Open source . | Neamtiu, I., Xie, G., Chen, J. (2013). Towards a better understanding of software evolution: an empirical study on open-source software. Journal of Software: Evolution and Process, 25(3), 193-218. | Comino, S., Manenti, F. M. (2011). Dual licensing in open source software markets. Information Economics and Policy, 23(3-4), 234-242. | Capra, E., Francalanci, C., Merlo, F. (2008). An empirical study on the relationship between software design quality, development effort and governance in open source projects. IEEE Transactions on Software Engineering, 34(6), 765-782. | Tan, X., Zhou, M., Fitzgerald, B. (2020, June). Scaling open source communities: An empirical study of the Linux kernel. ACM/IEEE 42nd International Conference on Software Engineering (pp. 1222-1234). | Qiu, H. S., Li, Y. L., Padala, S., Sarma, A., Vasilescu, B. (2019). The signals that potential contributors look for when choosing open-source projects. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW), 1-29.. | J. Coelho and M. T. Valente, “Why modern open source projects fail,” in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017, (New York, NY, USA), p. 186–196, Association for Computing Machinery, 2017. | C. Mendez, H. S. Padala, Z. Steine-Hanson, C. Hilderbrand, A. Horvath, C. Hill, L. Simpson, N. Patil, A. Sarma, and M. Burnett, “Open source barriers to entry, revisited: A sociotechnical perspective,” in Proceedings of the 40th International Conference on Software Engineering, ICSE ’18, (New York, NY, USA), p. 1004–1015, Association for Computing Machinery, 2018. | X. Tan, M. Zhou, and B. Fitzgerald, “Scaling open source communities: An empirical study of the linux kernel,” in Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE ’20, (New York, NY, USA), p. 1222–1234, Association for Computing Machinery, 2020. | E. Dias, P. Meirelles, F. Castor, I. Steinmacher, I. Wiese, and G. Pinto, “What makes a great maintainer of open source projects?,” in Proceedings of the 43rd International Conference on Software Engineering, ICSE ’21, p. 982–994, IEEE Press, 2021. | T. Dey, A. Karnauch, and A. Mockus, “Representation of developer expertise in open source software,” in Proceedings of the 43rd International Conference on Software Engineering, ICSE ’21, p. 995–1007, IEEE Press, 2021. | D. Riehle, P. Riemer, C. Kolassa, and M. Schmidt, “Paid vs. volunteer work in open source,” in Proceedings of the Annual Hawaii International Conference on System Sciences, 01 2014. | . Testing . | Dirk Beyer, Adam J Chlipala, Thomas A Henzinger, Ranjit Jhala, and Rupak Majumdar. 2004. Generating tests from counterexamples. In Proceedings. 26th International Conference on Software Engineering. IEEE, 326–335. | Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. 2008. Klee: unassisted and automatic generation of high-coverage tests for complex systems programs.. In OSDI, Vol. 8. 209–224. | Koen Claessen and John Hughes. 2000. QuickCheck: a lightweight tool for random testing of Haskell programs. In Proceedings of the fifth ACM SIGPLAN international conference on Functional programming. 268-279. | George Fink and Matt Bishop. 1997. Property-based testing: a new approach to testing for assurance. ACM SIGSOFT Software Engineering Notes 22, 4 (1997), 74-80. | Thomas A Henzinger, Ranjit Jhala, Rupak Majumdar, and Gregoire Sutre. 2002. Lazy abstraction. In Proceedings of the 29th ACM SIGPLAN-SIGACT symposium on Principles of programming languages. 58-70. | Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed automated random testing. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation. 213–223. | “Web accessibility evaluation methods: A systematic review. Design, User Experience, and Usability”, Nuñez et al. | “A brief survey of current software engineering practices in continuous integration and automated accessibility testing”, Sane | “Aid: An automated detector for gender-inclusivity bugs in OSS Project Pages”, Chatterjee et al. | “Mobile accessibility guidelines adoption under the perspective of developers and designers”, de Almeida and Gama | “Evaluating responsive web design’s impact on Blind Users”, Nogueira et al. | “Accessibility of UI frameworks and libraries for programmers with visual impairments”, Pandey et al. | Lin, Jun-Wei, Reyhaneh Jabbarvand, and Sam Malek. “Test transfer across mobile apps through semantic mapping.” 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019. | Behrang, Farnaz, and Alessandro Orso. “Test migration between mobile apps with similar functionality.” 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019. | Mariani, Leonardo, et al. “Semantic matching of gui events for test reuse: are we there yet?.” Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis. 2021. | Bajammal, Mohammad, and Ali Mesbah. “Semantic Web Accessibility Testing via Hierarchical Visual Analysis.” 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021. | Walsh, Thomas A., Phil McMinn, and Gregory M. Kapfhammer. “Automatic detection of potential layout faults following changes to responsive web pages (N).” 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2015. | “Automated Model-Based Android GUI Testing Using Multi-Level GUI Comparison Criteria”, Baek and Bae | “Automated testing of virtual reality application interfaces.”, Bierbaum et al. | “On the efficiency of automated testing”, Böhme and Paul | Automated Accessibility Testing of Mobile Apps”, Eler et al. | Robert Nilsson and Jeff Offutt. 2007. Automated Testing of Timeliness : A Case Study. (2007). | T.Y Chen, S.C. Cheung, and S.M. Yiu. Metamorphic testing: a new approach for generating next test cases. | Kyle Dewey, Jared Roesh, and Ben Hardekopf. Language fuzzing using constraint logic programming. | Kyle Dewey, Jared Roesch, and Ben Hardekopf. Fuzzing the Rust Typechecker UsingCLP. | Sungjae Hwang, Sungho Lee, Jihoon Kim, and Sukyoung Ryu. JUSTGen: Effective Test Generation for Unspecified JNI Behaviors on JVMs. | Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. Finding and Understanding Bugs in C Compilers. | Alex Groce, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr. Swarm testing. In Proc. ISSTA, pages 78–88, July 2012 | Y. Jia and M. Harman, “An analysis and survey of the development of mutation testing,” IEEE Transactions on Software Engineering, vol. 37, no. 5. pp. 649–678, 2011. | G. Petrović, M. Ivanković, B. Kurtz, P. Ammann, and R. Just, “An industrial application of mutation testing: Lessons, challenges, and research directions,” in Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018, Jul. 2018, pp. 47–53. | G. Petrovic, M. Ivankovic, G. Fraser, and R. Just, “Practical Mutation Testing at Scale: A view from Google,” IEEE Transactions on Software Engineering, vol. 48, no. 10. Institute of Electrical and Electronics Engineers Inc., pp. 3900–3912, Oct. 01, 2022. | R. J. Lipton and F. G. Sayward, “Hints on test data selection: Help for the practicing programmer,” Computer (Long Beach Calif), vol. 11, no. 4, pp. 34–41, 1978. | M. Ivankovi, G. Petrovi, R. Just, and G. Fraser, “Code coverage at Google,” in ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Aug. 2019, pp. 955–963. | . DevOps . | “Towards Automating the AI Operations Lifecycle”, Arnold et al. | “Towards ML engineering: a brief history of TensorFlow Extended (TFX)”, Karmarkar et al. | Leonel Aguilar, David Dao, Shaoduo Gan, Nezihe Merve Gurel, Nora Hollenstein, Jiawei Jiang, Bojan Karlas, Thomas Lemmin, Tian Li, Yang Li, et al. Ease. ml: A lifecycle management system for mldev and mlops. Proc. of Innovative Data Systems Research, 2021. | Alex Serban, Koen van der Blom, Holger Hoos, and Joost Visser. Adoption and effects of software engineering best practices in machine learning. In Proceedings of the 14th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), pages 1–12, 2020. | Nadia Nahar, Shurui Zhou, Grace Lewis, and Christian Kästner. Collaboration challenges in building ml-enabled systems: Communication, documentation, engineering, and process. In Proceedings of the 44th International Conference on Software Engineering, pages 413–425, 2022. | Tongfei Chen, Jirı́ Navrátil, Vijay Iyengar, and Karthikeyan Shanmugam. Confidence scoring using whitebox meta-models with linear classifier probes. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1467–1475. PMLR, 2019. | Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019. | “Testing Practices for Infrastructure as Code”, Hasan et al. | Hummer, W., Rosenberg, F., Oliveira, F., &amp; Eilam, T. (2013). Testing Idempotence for Infrastructure as Code. In D. Eyers &amp; K. Schwan (Eds.), Middleware 2013 (pp. 368–388). Berlin, Heidelberg: Springer Berlin Heidelberg. | “Survey of DevOps Concepts and Challenges”, Leite et al. | “The Seven Sins: Security Smells in Infrastructure as Code Scripts”, Rahman et al. | “CI/CD Pipelines Evolution and Restructuring: A Qualitative and Quantitative Study”, Zampetti et al. | “An Evolutionary Study of Configuration Design and Implementation in Cloud Systems”, Zhang et al. | . Expertise and knowledge sharing . | R. Kavitha and M. Irfan Ahmed, “A knowledge management framework for agile software development teams,” in 2011 International Conference on Process Automation, Control and Computing, pp. 1–5, 2011. | S. Mirhosseini and C. Parnin, “Docable: Evaluating the executability of software tutorials,” in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conferencve and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, (New York, NY, USA), p. 375–385, Association for Computing Machinery, 2020. | A. Horvath, M. X. Liu, R. Hendriksen, C. Shannon, E. Paterson, K. Jawad, A. Macvean, and B. A. Myers, “Understanding how programmers can use annotations on documentation,” in Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI ’22, (New York, NY, USA), Association for Computing Machinery, 2022. | T. D. LaToza, M. Arab, D. Loksa, and A. J. Ko, “Explicit programming strategies,” Empirical Software Engineering, vol. 25, no. 4, p. 2416–2449, 2020. | “Social media and tacit knowledge sharing: Developing a conceptual model”, Panahi et al. | “An exploratory study of live-streamed programming”, Alaboudi and LaToza | “How social and communication channels shape and challenge a participatory culture in software development”, Storey et al. | “Analyzing user comments on YouTube coding tutorial videos”, Poché et al. | “Influence of social and technical factors for evaluating contribution in GitHub”, Tsay et al. | S. Baltes and S. Diehl, “Towards a theory of software development expertise,” ESEC/FSE 2018: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 187-200, 2018. | F. O. Bjørnson and T. Dingsøyr, “Knowledge management in software engineering: A systematic review of studied concepts, findings and research methods used,” Information and Software Technology, vol. 50, no. 11, pp. 1055-1068, 2008. | M. Zhou and A. Mockus, “Developer fluency: achieving true mastery in software projects,”FSE ‘10: Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering, pp. 137-146, 2010. | M. Caulo, B. Lin, G. Bavota, G. Scanniello and M. Lanza, “Knowledge Transfer in Modern Code Review,” ICPC ‘20: Proceedings of the 28th International Conference on Program | C. Treude and M.-A. Storey, “Effective communication of software development knowledge through community portals,” ESEC/FSE ‘11: Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, pp. 91-101, 2011. | . Security . | “Backstabber’s Knife Collection: A Review of Open-Source Software Supply Chain Attacks”, Ohm et al. | “Containing Malicious Package Updates in npm with a Lightweight Permission System”, Ferreira et al. | “On the impact of security vulnerabilities in the npm package dependency network”, Decan et al. | “Towards Measuring Supply Chain Attacks on Package Managers for Interpreted Languages”, Duan et al. | . Software engineering in specific domains . Data science . | “Managing Messes in Computational Notebooks”, Head et al. | ” Refactoring in Computational Notebooks”, Liu et al. | “Understanding and improving the quality and reproducibility of Jupyter notebooks”, Pimentel et al. | “Error identification strategies for Python Jupyter notebooks”, Robinson et al. | “A static analysis framework for data science notebooks”, Subotić et al. | “Novice Reflections on Debugging”, Whalley et al. | “Towards Effective Foraging by Data Scientists to Find Past Analysis Choices”, Kerry et al. | “Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis”, Wang et al. | Jeremiah W Johnson. Benefits and pitfalls of jupyter notebooks in the classroom. In Proceedings of the 21st Annual Conference on Information Technology Education, pages 32–37, 2020. | Luigi Quaranta, Fabio Calefato, and Filippo Lanubile. Eliciting best practices for collaboration with computational notebooks. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1):1–41, 2022. | . Interactive theorem provers . | A. W. Appel, “Coq’s vibrant ecosystem for verification engineering (invited talk),” in Proceedings of the 11th ACM SIGPLAN international conference on certified programs and proofs, 2022, pp. 2–11. | F. van Doorn, G. Ebner, and R. Y. Lewis, “Maintaining a library of formal mathematics,” in Intelligent computer mathematics: 13th international conference, CICM 2020, bertinoro, italy, july 26–31, 2020, proceedings, 2020, pp. 251–267. | K. Roe and S. Smith, “CoqPIE: An IDE aimed at improving proof development productivity: (Rough diamond),” in Interactive theorem proving: 7th international conference, ITP 2016, nancy, france, august 22-25, 2016, proceedings 7, 2016, pp. 491–499. | A. Celik, K. Palmskog, M. Parovic, E. J. G. Arias, and M. Gligoric, “Mutation analysis for coq,” in 2019 34th IEEE/ACM international conference on automated software engineering (ASE), 2019, pp. 539–551. | T. Ringer, R. Porter, N. Yazdani, J. Leo, and D. Grossman, “Proof repair across type equivalences,” in Proceedings of the 42nd ACM SIGPLAN international conference on programming language design and implementation, 2021, pp. 112–127. | . Healthcare . | Alshayban, A., Ahmed, I., &amp; Malek, S. (2020, June). Accessibility issues in android apps: State of affairs, sentiments, and ways forward. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 1323-1334). | Perera, H., Hussain, W., Whittle, J., Nurwidyantoro, A., Mougouei, D., Shams, R. A., &amp; Oliver, G. (2020, June). A study on the prevalence of human values in software engineering publications, 2015-2018. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (pp. 409-420). | Beede, E., Baylor, E., Hersch, F., Iurchenko, A., Wilcox, L., Ruamviboonsuk, P., &amp; Vardoulakis, L. M. (2020, April). A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. In Proceedings of the 2020 CHI conference on human factors in computing systems (pp. 1-12). | Garde, S., &amp; Knaup, P. (2006). Requirements engineering in health care: the example of chemotherapy planning in paediatric oncology. Requirements Engineering, 11, 265-278. | Sutcliffe, A., Sawyer, P., Liu, W., &amp; Bencomo, N. (2021, May). Investigating the potential impact of values on requirements and software engineering. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS) (pp. 39-47). IEEE. | . Program understanding . | “Stepping OCaml” | “Stepping lazy programs” | “Finding latent code errors via machine learning over program executions” | “Checking Inside the Black Box: Regression Testing Based on Value Spectra Differences” | “Finding Causes of Program Output with the Java Whyline” | . ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/#list-of-all-articles-selected-by-students",
    
    "relUrl": "/assessments/paper/#list-of-all-articles-selected-by-students"
  },"74": {
    "doc": "Reflection Paper",
    "title": "Reflection Paper",
    "content": "The structure of this class is designed to provide a broad survey of many topics in software engineering practice and research: . | Software Process | Modularity and Design | Mining Software Repositories | Open Source | Testing | Continuous Integration | Devops | Expertise and knowledge sharing | Security | Software engineering in specific domains | . The goal of the reflection paper is for you to select a topic of particular personal interest, identify five research articles (that are not required readings for this course), read them, and organize your reaction to the topic in a reflection paper. The reflection paper provides you to an opportunity to apply critical reasoning skills in the context of the research papers that we discuss in class, and to dig deeper into a topic of your own particular interest. This is not a literature review: the goal with this paper is for you to provide some editorial critiques of the work and express your own opinions, rather than to provide a fair and direct summary of the contents of each paper. The expected length of the reflection paper is roughly 2,500 words. There are three main deliverables for the reflection paper: . | Paper proposal | The reflection paper | A short presentation in class of your findings | . The objectives for the reflection paper are to: . | Compare and contrast different software testing and analysis approaches | Critically analyze research projects | Cultivate new ideas for research directions | . Once you have selected the papers that you will respond to, consider one (or more) of the following prompts: . | What are common themes or goals between these papers? Do they share similar assumptions (in terms of how software is designed and tested)? | How do these works complement each other? | Based on your own experiences developing software, do you believe that the evaluations in these papers provide sufficient evidence to draw the conclusions that are claimed? | If you have experience with other related works in software engineering, how do you think those works compare? | Are there next-steps that you think could be applied to extend the ideas presented in these papers? | Were there parts of some of these works that you thought were poorly explained, and could have been written in another way to be more accessible? | . You should consider these prompts as jumping-off points: feel free to respond to one or more of these questions in your paper, or alternatively, something completely different. The most important part is that your paper should convey your own reactions to the six papers that you have selected — demonstrating that you read the papers and thought about them. This is an individual assignment. The goal of this assignment is to provide your personal reflection to the topic of your choice. Papers that are substantially similar to each other (or to third-party content) will receive a score of 0, with no opportunity for a revision. If you have any questions or concerns of what constitutes plagiarism, please reach out to the instructor immediately. ",
    "url": "/CS4910-7580-Spring-2023/assessments/paper/",
    
    "relUrl": "/assessments/paper/"
  },"75": {
    "doc": "Policies",
    "title": "Grading",
    "content": "Your grade in this class will be evaluated based on three assessments: . | Participation (updated weekly) | Reflection Paper (due mid-term) | Implementation Project (due at end-of-term) | . For each of these three assessments, you will receive a grade of check (satisfactory), check minus (needs improvement), check plus (exemplary) or no credit. Final grades will be assigned as follows: . | To receive a final grade of A: Receive a grade of at least “check” on all three assessments | To receive a final grade of B: Receive a grade of at least “check” on at least two of the three assessments | To receive a final grade of C: Receive a grade of at least “check” on at least one of the three assessments | . Final grades in-between these marks (e.g. A-/B+/B- etc.) may be assigned at the instructor’s discretion. I will provide a detailed rubric for each of these three components of your grade at the start of the semester, and will provide you with regular opportunities for feedback. As detailed in the assessments page, participation grades may be improved throughout the semester, and the reflection paper may be revised and re-submited. I will provide you with early feedback on your final project, but since the project is due at the end of the term, it will not be possible to resubmit it. The project will be graded on both effort and results: projects that ultimately don’t work out as intended, but are accompanied by a detailed report explaining what didn’t work and your hypotheses for why that didn’t work are welcome. The overall philosophy behind this grading scheme is to create the most opportunities for students to engage with the course material, while minimizing stress and anxiety from high-stakes deliverables. Note that the grade of “check plus” makes no overall benefit on your final course grade. I hope that this course draws a wide range of students, including undergraduates and masters students whose goal is to apply for software engineering roles in industry, PhD students who are interested in this and adjacent research topics, and broadly, any students who are interested in learning more about these topics and engaging with computer science research. I offer the grade of “check plus” as an indication that you have, literally, gone above and beyond what would be expected for a student in this class: perhaps your reflection paper sparked a new research direction, or your implementation project became merged in a major, popular software engineering tool. These are amazing outcomes for this course, but are certainly, by no means expected of any student. Please consider the grade of “check plus” to mean something along the lines of “I would quite enthusiastically write you a reference letter describing your participation/paper/project in my class; you should consider taking this further.” Of course, I’m happy to write reference letters for all of my students — but the “check plus” is a way that I can signal to you, during the semester, that you are greatly exceeding expectations. The Good Faith Effort Standard . My assessment rubrics refer in multiple places to a “good faith effort.” For instance, students who submit a paper that shows a good faith effort, but does not receive a grade of “Check” will receive detailed feedback on the paper, and be given the option to resubmit the paper, with those revisions. This revision process can be repeated twice. This policy is rooted in the philosophy that, as long as students are trying their best, they should be rewarded for those efforts and provided with support to continue to improve. Ultimately, this is a process that rewards the effort that you, as a student, put in, separately from the result that you produce — and hence, there must be some working definition for what a “reasonable effort” entails. This is the “good faith effort” — and we can discuss examples in class of what might or might not constitute a good faith effort on an assignment. Abstractly, a “good faith effort” is one wherein you could back up the claim “I tried, and did the best with my knowledge at the time.” One objective point worth noting, however, is that this is a 4-credit-hour course, which, according to the US federal government should equate to 200 minutes per-week of instructions, plus 8-12 hours homework, or equivalent (although there is, of course, also an expected variance in how long each student spends on a class, some might spend more, and others might spend less). ",
    "url": "/CS4910-7580-Spring-2023/policies/#grading",
    
    "relUrl": "/policies/#grading"
  },"76": {
    "doc": "Policies",
    "title": "Classroom Environment",
    "content": "To create and preserve a classroom atmosphere that optimizes teaching and learning, all participants share a responsibility in creating a civil and non-disruptive forum for the discussion of ideas. Students are expected to conduct themselves at all times in a manner that does not disrupt teaching or learning. Your comments to others should be constructive and free from harassing statements. You are encouraged to disagree with other students and the instructor, but such disagreements need to respectful and be based upon facts and documentation (rather than prejudices and personalities). The instructors reserve the right to interrupt conversations that deviate from these expectations. Repeated unprofessional or disrespectful conduct may result in a lower grade or more severe consequences. Part of the learning process in this course is respectful engagement of ideas with others. Please don’t be late. You are an essential part of the class. Your participation is an essential part of the class. If you are late, please be courteous to others when entering. Attendance in the class meetings is expected. Sometimes you cannot avoid missing a class. If you need to be away from class, it is your responsibility to catch up on the materials discussed in the class. Please see the course policy on remote attendance . ",
    "url": "/CS4910-7580-Spring-2023/policies/#classroom-environment",
    
    "relUrl": "/policies/#classroom-environment"
  },"77": {
    "doc": "Policies",
    "title": "Accommodations for Disabilities",
    "content": "Students who have disabilities who wish to receive academic services and/or accommodations should visit the Disability Resource Center at 20 Dodge Hall or call (617) 373-2675. If you have already done so, please provide your letter from the DRC to the Instructor early in the semester so that they can arrange those accommodations. ",
    "url": "/CS4910-7580-Spring-2023/policies/#accommodations-for-disabilities",
    
    "relUrl": "/policies/#accommodations-for-disabilities"
  },"78": {
    "doc": "Policies",
    "title": "Title IX Notice",
    "content": "Title IX of the Education Amendments of 1972 protects individuals from sex or gender-based discrimination, including discrimination based on gender-identity, in educational programs and activities that receive federal financial assistance. Northeastern’s Title IX Policy prohibits Prohibited Offenses, which are defined as sexual harassment, sexual assault, relationship or domestic violence, and stalking. The Title IX Policy applies to the entire community, including students, faculty and staff of all gender identities. If you or someone you know has been a survivor of a Prohibited Offense, confidential support and guidance can be found through University Health and Counseling Services staff and the Center for Spiritual Dialogue and Service clergy members. By law, those employees are not required to report allegations of sex or gender-based discrimination to the University. Alleged violations can be reported non-confidentially to the Title IX Coordinator within The Office for Gender Equity and Compliance at: titleix@northeastern.edu and/or through NUPD (Emergency 617.373.3333; Non-Emergency 617.373.2121). Reporting Prohibited Offenses to NUPD does NOT commit the victim/affected party to future legal action. Faculty members are considered “responsible employees” at Northeastern University, meaning they are required to report all allegations of sex or gender-based discrimination to the Title IX Coordinator. In case of an emergency, please call 911. Please visit www.northeastern.edu/ouec for a complete list of reporting options and resources both on- and off-campus. ",
    "url": "/CS4910-7580-Spring-2023/policies/#title-ix-notice",
    
    "relUrl": "/policies/#title-ix-notice"
  },"79": {
    "doc": "Policies",
    "title": "Policies",
    "content": " ",
    "url": "/CS4910-7580-Spring-2023/policies/",
    
    "relUrl": "/policies/"
  },"80": {
    "doc": "Preliminary Project Proposal",
    "title": "Preliminary Project Proposal",
    "content": "The project will involve a hands-on application of the techniques and tools that we discuss in class, and can be completed either individually or in a group of at most three. You can read more about the overall goals and structure for the project on the project writeup. Before the preliminary project proposal is due, we will dedicate an entire class meeting to brainstorming and discussing potential project topics. On the day that the preliminary proposal is due, we will have another discussion section in class to help refine project concepts, and then you will submit a revised project proposal. This should provide ample opportunities to resolve any concerns regarding project scope, feasibiltiy, and organization before the implementation phase beings. The project proposal should explain, at a high level, the project that you have in mind. The proposal should address the following points: . | Describe the goals of your project: are you seeking to develop new functionality for an existing application, develop a greenfield application, or evaluate some existing system? | What will be the concrete deliverables that you create? | If there will be an evaluation aspect, what metrics will you capture? | What technologies will you use? | What are the major risks that you see in the project that you are proposing? Do you have contingency plans in case some key aspect of the project doesn’t work out? | (If a team project): What are the high level roles and responsibilities of each team member? It is often helpful to have some notion of task ownership, rater than a “we are all responsible for everything” | . The proposal should be roughly one to two pages long, although if you find it useful to include figures, or additional text, that is OK, too. The preliminary project proposal is due on February 23rd at 11:00am, submitted via Canvas. If you are working in a group, please be sure to tag each of your team members on Canvas. A good-faith, on-time attempt to answer the questions above is necessary to receive a “check” on the project overall (the grading for the preliminary proposal is binary: either you have done it or you haven’t). ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-proposal/",
    
    "relUrl": "/assessments/project-proposal/"
  },"81": {
    "doc": "Project Showcase",
    "title": "In-App Testing",
    "content": ". ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-showcase#in-app-testing",
    
    "relUrl": "/assessments/project-showcase#in-app-testing"
  },"82": {
    "doc": "Project Showcase",
    "title": "Infrastructure for Inference on LLMs",
    "content": ". ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-showcase#infrastructure-for-inference-on-llms",
    
    "relUrl": "/assessments/project-showcase#infrastructure-for-inference-on-llms"
  },"83": {
    "doc": "Project Showcase",
    "title": "Example Garden",
    "content": ". ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-showcase#example-garden",
    
    "relUrl": "/assessments/project-showcase#example-garden"
  },"84": {
    "doc": "Project Showcase",
    "title": "Learning Fuzzing",
    "content": ". ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-showcase#learning-fuzzing",
    
    "relUrl": "/assessments/project-showcase#learning-fuzzing"
  },"85": {
    "doc": "Project Showcase",
    "title": "Project Showcase",
    "content": "This project showcase highlights the project posters that students agreed could be released to the public. Click on any poster to view it full size. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-showcase",
    
    "relUrl": "/assessments/project-showcase"
  },"86": {
    "doc": "Project Status Update",
    "title": "Project Status Update",
    "content": "The project status update should reflect on the progress that you have made thus far and outline the work that remains to be done on your project. The status update should address the following points: . | What have you accomplished? Include a link to the code that you have written so far. | What challenges have you encountered? | Are there any difficulties that are currently preventing you from making progress? | What tasks do you still need to complete? | Do you believe that your original proposal is still feasible? If no, what modifications to your original proposal would make it more feasible. | . The project status update should be roughly 2-4 paragraphs long. The project status update is due on March 28th at 11:00am, submitted via Canvas. If you are working in a group, please be sure to tag all of your group members in the submission form. A good-faith, on-time attempt to answer the questions above is necessary to receive a “check” on the project overall. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project-status-update/",
    
    "relUrl": "/assessments/project-status-update/"
  },"87": {
    "doc": "Implementation Project",
    "title": "Sample project ideas",
    "content": ". | Mining GitHub or other open source ecosystems to answer a software engineering research question | Conducting an evaluation of existing software engineering tools or frameworks in a new context (compared to existing evaluations) | Implementing new features for open source software engineering tools, for example: extensions to mutation testing tools, fuzzers, plugins for continuous integration platforms, etc. | Creating continuous integration workflows for automated performance testing of the covey.town platform | Applying continuous integration and automated testing to an existing research project | . We will add to this list throughout the first few weeks of the semester, and students are welcomed to propose project ideas that aren’t from this list. Course projects that build on or extend students’ projects from outside of the course are welcomed. You will have five primary deliverables for the project: 1) a preliminary proposal outlining your project, 2) a revised proposal, 3) a [mid-semester status update]](/CS4910-7580-Spring-2023/assessments/project-status-update/), 4) a final report describing your project experience, and 5) the actual software artifact that you created in your project. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/#sample-project-ideas",
    
    "relUrl": "/assessments/project/#sample-project-ideas"
  },"88": {
    "doc": "Implementation Project",
    "title": "The Project Report",
    "content": "In addition to producing a code artifact (e.g. your project implementation), you will also write up a final project report that describes your experience implementing and evaluating your project. The maximum length of the project report is ten pages, not including figures and/or references. As they say, “a picture is worth a thousand words” — we expect that there will be at least some figures in every project report (e.g. architectural diagrams, screenshots, evaluation results, etc.). Please consider ten pages as an absolute upper-bound on length: shorter may be better than longer, and a 3-5 page report that contains everything that we need to know is fine. The final report should contain the following sections (feel free to add additional sections; the sections below are required): . Introduction . Describe the project that you proposed (assuming that the reader of this report hasn’t read your project proposal). It is fine to copy and paste text from the project proposal here, if that text still applies. However, if you have made changes to your project topic based on feedback on the proposal, or based on experiences working on the project, be sure to describe here what you actually did, and not just what you originally planned to do. Project Results . Considering the overall project goals that you proposed, present the results or findings of your project, describing in detail what you learned from doing the project. Describe the technologies that you used, and how you used them. In this section, focus on goals that you successfully achieved, or that failing, concepts and technologies that you learned. If your project builds upon existing code, frameworks, scripts, tools, etc. built by you or someone else, be sure to clearly differentiate the work that you implemented for this project in this course, as opposed to work you or others may have done previously. Project Challenges . While it is possible that you successfully implemented your entire project without any challenges, it’s quite likely that you ran into a variety of challenges along the way. In some cases, you may have even discovered that the overall goals that you set out to achieve are simply not attainable (or at least, not attainable in the time frame allocated). That is absolutely fine, and for better or worse, is the price of working on research problems: if we knew the answer to these problems already, they wouldn’t be research! . In this section, you should describe in detail challenges that you encountered, and what you did to try to work around them. If you ran into implementation-level challenges (e.g. difficulties with particular technologies, unescapable segfaults, etc) describe your hypothesis for why you might have run into this challenge, and any steps that you have taken to try to test that hypothesis/debug the situation. This section provides an opportunity for a project that has no working implementation at all to still receive a grade of “check” - if you could never get the project to work, but can describe various steps that you took to debug your project, you are still creating an artifact that demonstrates your understanding of advanced software engineering processes. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/#the-project-report",
    
    "relUrl": "/assessments/project/#the-project-report"
  },"89": {
    "doc": "Implementation Project",
    "title": "Project Poster Presentation",
    "content": "Create a poster that summarizes your project, including the introduction/motivation, the results, and challenges. We have no specific formatting requirements for the poster, but encourage you to use the space primarily for figures/images/tables rather than for blocks of text. To encourage interactivity, posters will be presented in two ways: . | Asynchronously, via Discord: We will share all posters on Discord before class on the 18th. Students can review and comment on posters before they are presented. | During class time on the 18th and 20th, each group will provide a ~5-7 minute presentation of their poster (we will present the posters on the screen) and answer questions. | . We would like to collect posters and highlight them on the course website, allowing future students to get a sense for the kinds of projects that students have undertaken in the past. When we have done this in other classes, students have also later remarked that it was helpful for their project to be highlighted on the class website, as it provided a reputable URL that could be shared with potential employers or collaborators to simultaneously present the student’s work and contextualize it within the course. Please include a statement when you upload your poster as to whether you consent to the poster being shared publicly or not. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/#project-poster-presentation",
    
    "relUrl": "/assessments/project/#project-poster-presentation"
  },"90": {
    "doc": "Implementation Project",
    "title": "Final Project Submission",
    "content": "Create a zip file with your entire implementation artifact. Create a PDF with your report, and a separate PDF with your poster. Submit all three on Canvas by April 18 at 11:00am (use the assignment labeled “Implementation Project” for the implementation itself, “Report” for the report and “Poster” for the poster). No late submissions will be accepted. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/#final-project-submission",
    
    "relUrl": "/assessments/project/#final-project-submission"
  },"91": {
    "doc": "Implementation Project",
    "title": "Grading Rubric",
    "content": "Your project will be graded on the scale of (Unacceptable, Check-, Check, Check+). The criteria for each grade are described below: . To receive the grade of Check, the project must satisfy all of these criteria: . | The project proposal is submitted on time. The proposal is roughly one page long and includes a good-faith effort to address each of the 5 points outlined above. | The project status update is submitted on time. The status update is roughly 2-4 paragraphs long and includes a good-faith effort to address each of the 5 points outlined above. | The project includes a response to any feedback provided on the proposal: for any changes requested by the instructor, either those changes were accepted, or there is a discussion in the report of why those changes did not make sense. | The project report is between 3-10 pages long, contains at least the three sections outlined above, and captures your efforts on implementing your project | The project report and implementation are submitted on time | The project poster is submitted on time and summarizes the introduction/motivation to the project, the results, and any signfiicant challenges encountered | EITHER: . | 1) the project implementation successfully achieves the goals outlined, OR: | 2) the “Project Challenges” section of the report describes in detail the challenges that you faced that prevented you from achieving some goal(s), and the steps that you took to try to overcome those challenges. The record of steps that you took to overcome those challenges is sufficiently detailed to demonstrate that you put a good-faith effort into getting it to work. The project status update supports this narrative. | . | . To receive the grade of Check-: the project must satisfy all of these criteria: . | The project proposal is submitted on time. The proposal is roughly one page long and includes a good-faith effort to address each of the 5 points outlined above. | The project status update is submitted on time. The status update is roughly 2-4 paragraphs long and includes a good-faith effort to address each of the 5 points outlined above. | The project report is between 3-10 pages long, contains at least the three sections outlined above, with a good-faith effort to respond to the prompts. | The project report, implementation and/or poster are submitted on time | The project implementation may not achieve all of the goals outlined, the project challenges section may not include a detailed discussion of the challenges encountered and good-faith attempts to overcome those challenges. | . Submissions that do not meet the criteria for “Check-“ will receive the grade of “Unacceptable.” Submissions that exceed the qualities outlined for “Check” may receive the grade of “Check+”. As per the course policies, note that a grade of “Check” is sufficient to receive an A in the class. ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/#grading-rubric",
    
    "relUrl": "/assessments/project/#grading-rubric"
  },"92": {
    "doc": "Implementation Project",
    "title": "Implementation Project",
    "content": "The project will involve a hands-on application of the techniques and tools that we discuss in class, and can be completed either individually or in a group of at most three. Topics for the project will be discussed in the first six weeks of class, and your specific project topic will be finalized soon after. Both research-oriented projects (which implement and evaluate some new idea) and engineering-oriented projects (which make contributions to tools that developers may already use) are welcomed. All projects must involve the implementation of some software artifact (e.g. building a new system, extending an existing one, or developing scripting to automate the evaluation of some system). ",
    "url": "/CS4910-7580-Spring-2023/assessments/project/",
    
    "relUrl": "/assessments/project/"
  },"93": {
    "doc": "Other Resources",
    "title": "Other Resources",
    "content": "There are many articles, blogs, books and podcasts that are very interesting for more reading on the topics discussed in class. We’ll update this list as the semester goes, and if you have suggestions of materials to share, please let us know and we’ll add it to the list. Research mechanics . | “How to Read an engineering research paper” by William Griswold | . Contributing to open source projects . | “How to create and review a GitHub pull request” by Mike Ernstt | How to write a good bug report: Oracle , Mozilla , Ximian , Tatham , Raymond , Software Testing Help (all via Mike Ernst’s advice page) | . General SE Knowledge . | “Software Engineering at Google” provides an excellent primer on the practices and processes of software engineering at Google | . Podcasts . The Software Engineering Radio podcast (also available wherever you get your podcasts) aims to produce educational material for professional softare developers, and includes conversations between experts and researchers on various software engineering topics. Here are a few of our favorites that are most relevant to topics that we cover in this class: . | Donny Nadolny on Debugging Distributed Systems, with Robert Blumen, 2017 | Jafar Soltani on Continuous Delivery for Multiplayer Games, with Nate Black, 2018 | Chris Richardson on Microservice Patterns, with Robert Blumen, 2019 | Margaret Burnett on Gender, Cognitive Styles and Usability Bugs, with Felienne Hermans, 2019 | Michaela Greiler on Code Reviews, with Felienne Hermans, 2020 | Ipek Ozkaya on Managing Technical Debt, 2021 | . Code style . | Research paper: To camelcase or under_score, Dave Binkley et al | Book (free via library): “Refactoring: Improving the Design of Existing Code” By Martin Fowler The definitive list of “code smells” that should be avoiding in programming, matched up with “refactoring” techniques to improve that code | . Debugging . | Book (free via library): “Effective Debugging: 66 Specific Ways to Debug Software and Systems” by Diomidis SpinellisA helpful guidebook for debugging, laying out different strategies that are effective for testing different kinds of debugging hypotheses. | . Architecture and Design . | “Thinking Like a Software Architect” by Christian Kästner | “Fundamentals of Software Architecture” By Mark Richards and Neal Ford | Book (free via library): “Design Patterns Explained: A New Perspective on Object-Oriented Design” by Alan Shalloway and James TrottIn-depth coverage of design patterns, considering why they are important in software development, how to apply them, and descriptions of some common patterns. | . Infrastructure + Operations . | Book (free via library) “Site Reliability Engineering” By Betsy Beyer, Chris Jones, Niall Richard Murphy, Jennifer Petoff Site Reliability Engineering is a topic very related to software engineering: while software engineers might focus primarily on the design and development of software systems, SRE’s are engineers who focus on the deployment, monitoring and maintenance of that software. This book documents the SRE practices at Google. | . Technical Debt . | Book (free via library): “Managing Technical Debt: Reducing Friction in Software Development” by Philippe Kruchten, Robert Nord and Ipek Ozkaya | . Program Understanding . | Book: “The Programmer’s Brain” by Felienne Hermans A survey of research on programming and cognition, covering topics such as how to read code more effectively and how to write code that is easier to read and share. | . Evaluations . | The ACM SIGPLAN’s “Empirical Evaluation Guidelines” | “A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering” by Andrea Arcuri and Lionel Briand | . ",
    "url": "/CS4910-7580-Spring-2023/resources/",
    
    "relUrl": "/resources/"
  },"94": {
    "doc": "Revised Project Proposal",
    "title": "Revised Project Proposal",
    "content": "The project will involve a hands-on application of the techniques and tools that we discuss in class, and can be completed either individually or in a group of at most three. You can read more about the overall goals and structure for the project on the project writeup. Using the feedback provided by the course staff on your preliminary proposal and informed by the project discussions that we have in class, revise your project proposal. You may make any revisions that you feel are necessary, up to an extreme of working on a totally different project (perhaps even in a different team). At the other extreme: if there are no changes whatsoever, then include a note stating so. The format for the revised project proposal is identical to the preliminary one. The project proposal should explain, at a high level, the project that you have in mind. The proposal should address the following points: . | Describe the goals of your project: are you seeking to develop new functionality for an existing application, develop a greenfield application, or evaluate some existing system? | What will be the concrete deliverables that you create? | If there will be an evaluation aspect, what metrics will you capture? | What technologies will you use? | What are the major risks that you see in the project that you are proposing? Do you have contingency plans in case some key aspect of the project doesn’t work out? | (If a team project): What are the high level roles and responsibilities of each team member? It is often helpful to have some notion of task ownership, rater than a “we are all responsible for everything” | New compared to preliminary proposal: What are the high-level tasks that you will need to complete in order to accomplish this project? By when do you plan to accomplish each? Note that you will need to provide a status update on your project on Mar 28 - it may be wise to ensure that any particularly high-risk tasks are scheduled to be completed before the status update, such that we can use that checkpoint to discuss alternatives as needed. | . The proposal should be roughly one to two pages long, although if you find it useful to include figures, or additional text, that is OK, too. The revised project proposal is due on Mar 2 at 11:00am, submitted via Canvas. A good-faith, on-time attempt to answer the questions above is necessary to receive a “check” on the project overall (the grading for the preliminary proposal is binary: either you have done it or you haven’t). ",
    "url": "/CS4910-7580-Spring-2023/assessments/revised-project-proposal/",
    
    "relUrl": "/assessments/revised-project-proposal/"
  }
}
